{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "862301a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c719aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test.csv')\n",
    "train = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Indicator for missing values in rows\n",
    "train['missing_indicator'] = train.isnull().any(axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0cb456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "market_forward_excess_returns",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "349a4b8f-84e3-4bc2-8a35-0825974793f3",
       "rows": [
        [
         "count",
         "8990.0"
        ],
        [
         "mean",
         "5.054538444537134e-05"
        ],
        [
         "std",
         "0.010567748849696875"
        ],
        [
         "min",
         "-0.0405819137459235"
        ],
        [
         "25%",
         "-0.004759331863564825"
        ],
        [
         "50%",
         "0.00025175312094985003"
        ],
        [
         "75%",
         "0.0054794648032180995"
        ],
        [
         "max",
         "0.0405510761962616"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "count    8990.000000\n",
       "mean        0.000051\n",
       "std         0.010568\n",
       "min        -0.040582\n",
       "25%        -0.004759\n",
       "50%         0.000252\n",
       "75%         0.005479\n",
       "max         0.040551\n",
       "Name: market_forward_excess_returns, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train['market_forward_excess_returns'].describe())\n",
    "\n",
    "plt.hist(train['market_forward_excess_returns'], bins=50)\n",
    "plt.title('Histogram of Market Forward Excess Return')\n",
    "plt.xlabel('Market Forward Excess Return')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85241268",
   "metadata": {},
   "source": [
    "### Train has many cols with missing data\n",
    "- They are for earlier dates - potentially moving averages or similar?\n",
    "- The last 22% of data (2021 rows) have full data\n",
    "\n",
    "Option 1:\n",
    "- Impute using various methods\n",
    "- Backfill, mean, median and KNN\n",
    "\n",
    "Option 2:\n",
    "- Don't inpute and use methods which handle NaN values\n",
    "- E.g. xgboost and other tree methods\n",
    "\n",
    "Plan:\n",
    "- Try both option 1 (with the various imputers) and option 2\n",
    "- Test on first 80% of data by date_id, test on 80-90% percentile of date_id\n",
    "- Test on first 90% of data by date_id, test on remaining 10%\n",
    "- Compare out of sample performance metrics, with R^2 being more relevant here\n",
    "\n",
    "Additional thoughts:\n",
    "- Start with simple models then experiment further - however tree based models are good to start for feature importance\n",
    "- Consider weighting the data used for fitting by a time decay (more recent data more useful) - only after fitting with no time decay\n",
    "- Look at feature reduction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c37234c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 1. Backfill (future → past) ---\n",
    "train_backfill = train.copy()\n",
    "train_backfill = train_backfill.bfill().ffill()   # ffill after bfill to handle edges\n",
    "\n",
    "\n",
    "# --- 2. Global Mean Imputation ---\n",
    "train_mean_impute = train.copy()\n",
    "train_mean_impute = train_mean_impute.fillna(train_mean_impute.mean())\n",
    "\n",
    "\n",
    "# --- 3. Global Median Imputation ---\n",
    "train_median_impute = train.copy()\n",
    "train_median_impute = train_median_impute.fillna(train_median_impute.median())\n",
    "\n",
    "# --- 4. KNN Imputation with Scaling ---\n",
    "# Identify numeric columns excluding specific ones\n",
    "numeric_cols = train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cols_exclude = [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"market_forward_excess_returns\", \"missing_indicator\"] \n",
    "cols_for_knn = [c for c in numeric_cols if c not in cols_exclude]\n",
    "\n",
    "# Copy for reconstruction\n",
    "train_knn_impute = train.copy()\n",
    "\n",
    "# Extract only the columns that should be imputed\n",
    "X = train[cols_for_knn].copy()\n",
    "\n",
    "# --- Scaling ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# --- KNN Imputation ---\n",
    "knn = KNNImputer(n_neighbors=2)\n",
    "X_scaled_imputed = knn.fit_transform(X_scaled)\n",
    "\n",
    "# --- Inverse scaling ---\n",
    "X_imputed = scaler.inverse_transform(X_scaled_imputed)\n",
    "\n",
    "# --- Reassemble ---\n",
    "train_knn_impute[cols_for_knn] = X_imputed\n",
    "\n",
    "# --- Rounding issue of Binary Columns ---\n",
    "binary_cols = [col for col in cols_for_knn if col.startswith(\"D\")]\n",
    "for col in binary_cols:\n",
    "    train_knn_impute[col] = train_knn_impute[col].round().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd697e2",
   "metadata": {},
   "source": [
    "- When training on the imputed data to test on the non imputed data, backfill is best as many features are rolling averages\n",
    "- This isn't too informative however, as we use the test set for training the imputed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82347a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# --- Helper: function to add target & split, train RF, and evaluate ---\n",
    "def evaluate_imputer_version(df_imputed, imputer_name):\n",
    "    df = df_imputed.copy()\n",
    "\n",
    "    # Ensure sorted by time\n",
    "    df = df.sort_values(\"date_id\")\n",
    "\n",
    "    # Create target: next day's market_forward_excess_returns\n",
    "    df[\"lagged_market_forward_excess_returns\"] = df[\"market_forward_excess_returns\"].shift(-1)\n",
    "\n",
    "    # Drop last row (no target)\n",
    "    df = df.dropna(subset=[\"lagged_market_forward_excess_returns\"])\n",
    "\n",
    "    # Columns to exclude from features\n",
    "    base_exclude = [\n",
    "        \"date_id\",\n",
    "        \"forwards_returns\",         \n",
    "        \"risk_free_rate\",\n",
    "        \"market_forward_excess_returns\",\n",
    "        \"missing_indicator\",\n",
    "        \"lagged_market_forward_excess_returns\",   # target\n",
    "    ]\n",
    "\n",
    "    # Use only numeric columns as features (avoids encoding)\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    exclude_cols = [c for c in base_exclude if c in numeric_cols]\n",
    "    feature_cols = [c for c in numeric_cols if c not in exclude_cols]\n",
    "\n",
    "    # Train on rows that originally had missing values; test on rows with no missing values\n",
    "    train_mask = df[\"missing_indicator\"] == 1\n",
    "    test_mask  = df[\"missing_indicator\"] == 0\n",
    "\n",
    "    X_train = df.loc[train_mask, feature_cols]\n",
    "    y_train = df.loc[train_mask, \"lagged_market_forward_excess_returns\"]\n",
    "\n",
    "    X_test  = df.loc[test_mask, feature_cols]\n",
    "    y_test  = df.loc[test_mask, \"lagged_market_forward_excess_returns\"]\n",
    "\n",
    "    # Simple, reasonably robust model with minimal tuning\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Metrics (focus on R² but also MAE/RMSE)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    return {\n",
    "        \"imputer\": imputer_name,\n",
    "        \"R2\": r2,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"model\": model,\n",
    "        \"feature_cols\": feature_cols,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- 2. Run for each imputed dataset ---\n",
    "\n",
    "imputed_versions = {\n",
    "    \"backfill\": train_backfill,\n",
    "    \"mean\": train_mean_impute,\n",
    "    \"median\": train_median_impute,\n",
    "    \"knn\": train_knn_impute,\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, df_imp in imputed_versions.items():\n",
    "    print(f\"Evaluating imputer: {name}\")\n",
    "    res = evaluate_imputer_version(df_imp, name)\n",
    "    results.append(res)\n",
    "\n",
    "# --- 3. Summarise metrics in a DataFrame ---\n",
    "\n",
    "metrics_df = pd.DataFrame(\n",
    "    [{\"imputer\": r[\"imputer\"], \"R2\": r[\"R2\"], \"MAE\": r[\"MAE\"], \"RMSE\": r[\"RMSE\"]} for r in results]\n",
    ").sort_values(\"R2\", ascending=False)\n",
    "\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872f46da",
   "metadata": {},
   "source": [
    "- Here we train and test on the imputed set only, so more informative\n",
    "- KNN best, so will be the imputation method used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec810f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_imputer_missing_only(df_imputed, imputer_name):\n",
    "    df = df_imputed.copy().sort_values(\"date_id\")\n",
    "\n",
    "    # Target: next-day excess return\n",
    "    df[\"lagged_market_forward_excess_returns\"] = (\n",
    "        df[\"market_forward_excess_returns\"].shift(-1)\n",
    "    )\n",
    "\n",
    "    df = df.dropna(subset=[\"lagged_market_forward_excess_returns\"])\n",
    "\n",
    "    # Only rows that were originally missing\n",
    "    df_sub = df[df[\"missing_indicator\"] == 1]\n",
    "\n",
    "    # Time split INSIDE missing region\n",
    "    split = int(len(df_sub) * 0.8)\n",
    "    train_df = df_sub.iloc[:split]\n",
    "    test_df  = df_sub.iloc[split:]\n",
    "\n",
    "    # Feature exclusion\n",
    "    base_exclude = [\n",
    "        \"date_id\",\n",
    "        \"forward_returns\",\n",
    "        \"risk_free_rate\",\n",
    "        \"market_forward_excess_returns\",\n",
    "        \"missing_indicator\",\n",
    "        \"lagged_market_forward_excess_returns\",\n",
    "    ]\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    feature_cols = [c for c in numeric_cols if c not in base_exclude]\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[\"lagged_market_forward_excess_returns\"]\n",
    "\n",
    "    X_test  = test_df[feature_cols]\n",
    "    y_test  = test_df[\"lagged_market_forward_excess_returns\"]\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    return {\n",
    "        \"imputer\": imputer_name,\n",
    "        \"R2\": r2_score(y_test, y_pred),\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "    }\n",
    "\n",
    "results_missing_only = []\n",
    "\n",
    "for name, df_imp in imputed_versions.items():\n",
    "    print(f\"Evaluating (missing-only regime): {name}\")\n",
    "    res = evaluate_imputer_missing_only(df_imp, name)\n",
    "    results_missing_only.append(res)\n",
    "\n",
    "missing_only_df = pd.DataFrame(results_missing_only).sort_values(\"R2\", ascending=False)\n",
    "print(missing_only_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a842f9",
   "metadata": {},
   "source": [
    "- Next two blocks to find best KNN tuning\n",
    "- The first one finds that the best KNN with 2-5 neighbours \n",
    "- The second code block now uses multiple time based splits to further indentify best KNN method\n",
    "- K=2 is chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd1e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Function: given an imputed df, evaluate on missing-only regime (stationary test) ---\n",
    "def evaluate_on_missing_only(df_imputed, label):\n",
    "    df = df_imputed.copy().sort_values(\"date_id\")\n",
    "\n",
    "    # Target: next-day market_forward_excess_returns\n",
    "    df[\"lagged_market_forward_excess_returns\"] = df[\"market_forward_excess_returns\"].shift(-1)\n",
    "    df = df.dropna(subset=[\"lagged_market_forward_excess_returns\"])\n",
    "\n",
    "    # Restrict to rows that originally had missing data\n",
    "    df_sub = df[df[\"missing_indicator\"] == 1].copy()\n",
    "\n",
    "    # Time split inside missing-only region\n",
    "    split = int(len(df_sub) * 0.8)\n",
    "    train_df = df_sub.iloc[:split]\n",
    "    test_df  = df_sub.iloc[split:]\n",
    "\n",
    "    # Exclude non-feature columns\n",
    "    base_exclude = [\n",
    "        \"date_id\",\n",
    "        \"forward_returns\",\n",
    "        \"risk_free_rate\",\n",
    "        \"market_forward_excess_returns\",\n",
    "        \"missing_indicator\",\n",
    "        \"lagged_market_forward_excess_returns\",\n",
    "    ]\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    feature_cols = [c for c in numeric_cols if c not in base_exclude]\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[\"lagged_market_forward_excess_returns\"]\n",
    "\n",
    "    X_test  = test_df[feature_cols]\n",
    "    y_test  = test_df[\"lagged_market_forward_excess_returns\"]\n",
    "\n",
    "    # Simple, stable model\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=300,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    return {\n",
    "        \"knn_setting\": label,\n",
    "        \"R2\": r2,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "    }\n",
    "\n",
    "\n",
    "k_values = [1, 2, 3, 4, 5, 7, 10, 15, 25]\n",
    "\n",
    "weight_options = [\"uniform\", \"distance\"]\n",
    "\n",
    "results_knn_refined = []\n",
    "\n",
    "for weights in weight_options:\n",
    "    for k in k_values:\n",
    "        print(f\"KNN: k={k}, weights={weights}\")\n",
    "\n",
    "        train_knn_impute_k = train.copy()\n",
    "        X = train[cols_for_knn].copy()\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        knn = KNNImputer(n_neighbors=k, weights=weights)\n",
    "        X_scaled_imputed = knn.fit_transform(X_scaled)\n",
    "\n",
    "        X_imputed = scaler.inverse_transform(X_scaled_imputed)\n",
    "        train_knn_impute_k[cols_for_knn] = X_imputed\n",
    "\n",
    "        binary_cols = [c for c in cols_for_knn if c.startswith(\"D\")]\n",
    "        for col in binary_cols:\n",
    "            train_knn_impute_k[col] = train_knn_impute_k[col].round().astype(int)\n",
    "\n",
    "        res = evaluate_on_missing_only(\n",
    "            train_knn_impute_k,\n",
    "            label=f\"k={k}, w={weights}\"\n",
    "        )\n",
    "\n",
    "        results_knn_refined.append(res)\n",
    "\n",
    "knn_refined_df = (\n",
    "    pd.DataFrame(results_knn_refined)\n",
    "      .sort_values(\"R2\", ascending=False)\n",
    ")\n",
    "\n",
    "print(knn_refined_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aecf59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1. Evaluation on MULTIPLE time splits (missing-only regime)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def evaluate_knn_multi_split(df_imputed, label, split_fracs=[0.6, 0.7, 0.8, 0.85]):\n",
    "\n",
    "    df = df_imputed.copy().sort_values(\"date_id\")\n",
    "\n",
    "    # Target: next-day excess return\n",
    "    df[\"lagged_market_forward_excess_returns\"] = (\n",
    "        df[\"market_forward_excess_returns\"].shift(-1)\n",
    "    )\n",
    "    df = df.dropna(subset=[\"lagged_market_forward_excess_returns\"])\n",
    "\n",
    "    # Only missing-only regime\n",
    "    df_sub = df[df[\"missing_indicator\"] == 1].copy()\n",
    "\n",
    "    base_exclude = [\n",
    "        \"date_id\", \"forward_returns\", \"risk_free_rate\",\n",
    "        \"market_forward_excess_returns\",\n",
    "        \"missing_indicator\",\n",
    "        \"lagged_market_forward_excess_returns\",\n",
    "    ]\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    feature_cols = [c for c in numeric_cols if c not in base_exclude]\n",
    "\n",
    "    r2_list, mae_list, rmse_list = [], [], []\n",
    "\n",
    "    for frac in split_fracs:\n",
    "        split = int(len(df_sub) * frac)\n",
    "        train_df = df_sub.iloc[:split]\n",
    "        test_df  = df_sub.iloc[split:]\n",
    "\n",
    "        X_train = train_df[feature_cols]\n",
    "        y_train = train_df[\"lagged_market_forward_excess_returns\"]\n",
    "\n",
    "        X_test  = test_df[feature_cols]\n",
    "        y_test  = test_df[\"lagged_market_forward_excess_returns\"]\n",
    "\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=300,\n",
    "            min_samples_leaf=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        r2_list.append(r2_score(y_test, y_pred))\n",
    "        mae_list.append(mean_absolute_error(y_test, y_pred))\n",
    "        rmse_list.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "    return {\n",
    "        \"knn_setting\": label,\n",
    "        \"R2_mean\": np.mean(r2_list),\n",
    "        \"R2_std\":  np.std(r2_list),\n",
    "        \"MAE_mean\": np.mean(mae_list),\n",
    "        \"RMSE_mean\": np.mean(rmse_list),\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Run stability test for k = 2, 3, 4, 5 (uniform)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "k_values_final = [1, 2, 3, 4, 5, 10]\n",
    "\n",
    "results_knn_stability = []\n",
    "\n",
    "for k in k_values_final:\n",
    "    print(f\"Stability test for KNN: k={k}, weights=uniform\")\n",
    "\n",
    "    # ---- KNN Imputation ----\n",
    "    train_knn_k = train.copy()\n",
    "\n",
    "    X = train[cols_for_knn].copy()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    knn = KNNImputer(n_neighbors=k, weights=\"uniform\")\n",
    "    X_scaled_imputed = knn.fit_transform(X_scaled)\n",
    "\n",
    "    X_imputed = scaler.inverse_transform(X_scaled_imputed)\n",
    "    train_knn_k[cols_for_knn] = X_imputed\n",
    "\n",
    "    # Fix binary columns\n",
    "    binary_cols = [c for c in cols_for_knn if c.startswith(\"D\")]\n",
    "    for col in binary_cols:\n",
    "        train_knn_k[col] = train_knn_k[col].round().astype(int)\n",
    "\n",
    "    # ---- Multi-split evaluation ----\n",
    "    res = evaluate_knn_multi_split(\n",
    "        train_knn_k,\n",
    "        label=f\"k={k}, w=uniform\"\n",
    "    )\n",
    "\n",
    "    results_knn_stability.append(res)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Final stability table\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "knn_stability_df = (\n",
    "    pd.DataFrame(results_knn_stability)\n",
    "      .sort_values(\"R2_mean\", ascending=False)\n",
    ")\n",
    "\n",
    "print(knn_stability_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc08efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Helper: evaluate a given dataset (imputed or raw) with the same model + split\n",
    "def evaluate_full_train_dataset(df_input, method_name):\n",
    "    df = df_input.copy().sort_values(\"date_id\")\n",
    "\n",
    "    # Target: next-day market_forward_excess_returns\n",
    "    df[\"lagged_market_forward_excess_returns\"] = (\n",
    "        df[\"market_forward_excess_returns\"].shift(-1)\n",
    "    )\n",
    "\n",
    "    # Drop last row (no target) or any NaN target\n",
    "    df = df.dropna(subset=[\"lagged_market_forward_excess_returns\"])\n",
    "\n",
    "    # Exclude non-feature columns\n",
    "    base_exclude = [\n",
    "        \"date_id\",\n",
    "        \"forward_returns\",\n",
    "        \"risk_free_rate\",\n",
    "        \"market_forward_excess_returns\",\n",
    "        \"missing_indicator\",\n",
    "        \"lagged_market_forward_excess_returns\",\n",
    "    ]\n",
    "\n",
    "    # Use only numeric features\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    feature_cols = [c for c in numeric_cols if c not in base_exclude]\n",
    "\n",
    "    # Chronological 80/20 split\n",
    "    split_idx = int(len(df) * 0.8)\n",
    "    train_df = df.iloc[:split_idx]\n",
    "    test_df  = df.iloc[split_idx:]\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[\"lagged_market_forward_excess_returns\"]\n",
    "\n",
    "    X_test  = test_df[feature_cols]\n",
    "    y_test  = test_df[\"lagged_market_forward_excess_returns\"]\n",
    "\n",
    "    # Same model for all cases; supports NaNs natively\n",
    "    model = HistGradientBoostingRegressor(\n",
    "        max_depth=None,\n",
    "        learning_rate=0.05,\n",
    "        max_iter=500,\n",
    "        l2_regularization=0.0,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    return {\n",
    "        \"method\": method_name,\n",
    "        \"R2\": r2,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"model\": model,\n",
    "        \"features\": feature_cols,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Build the different versions to compare ---\n",
    "\n",
    "datasets_to_eval = {\n",
    "    \"no_imputation_raw\": train,               # raw, with NaNs\n",
    "    \"backfill\":           train_backfill,\n",
    "    \"mean\":               train_mean_impute,\n",
    "    \"median\":             train_median_impute,\n",
    "    \"knn_k2\":             train_knn_impute,   # your tuned KNN (k=2)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, df_ver in datasets_to_eval.items():\n",
    "    print(f\"Evaluating method: {name}\")\n",
    "    res = evaluate_full_train_dataset(df_ver, name)\n",
    "    results.append(res)\n",
    "\n",
    "# Summarise metrics in a clean table (models kept in `results` if you need them)\n",
    "metrics_df = pd.DataFrame(\n",
    "    [{\"method\": r[\"method\"], \"R2\": r[\"R2\"], \"MAE\": r[\"MAE\"], \"RMSE\": r[\"RMSE\"]} for r in results]\n",
    ").sort_values(\"R2\", ascending=False)\n",
    "\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ffd2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# columns we don't want as features\n",
    "cols_exclude = [\n",
    "    \"date_id\",\n",
    "    \"forward_returns\",\n",
    "    \"risk_free_rate\",\n",
    "    \"market_forward_excess_returns\",\n",
    "    \"missing_indicator\",\n",
    "    \"lagged_market_forward_excess_returns\",\n",
    "    \"lagged_forward_returns\",\n",
    "    \"lagged_risk_free_rate\",\n",
    "    \"is_scored\",\n",
    "]\n",
    "\n",
    "def make_train_with_target(df):\n",
    "    df = df.copy().sort_values(\"date_id\")\n",
    "    df[\"lagged_market_forward_excess_returns\"] = df[\"market_forward_excess_returns\"].shift(-1)\n",
    "    df = df.dropna(subset=[\"lagged_market_forward_excess_returns\"])\n",
    "    return df\n",
    "\n",
    "def make_test_with_target(df):\n",
    "    df = df.copy().sort_values(\"date_id\")\n",
    "    if \"is_scored\" in df.columns:\n",
    "        df = df[df[\"is_scored\"] == 1]\n",
    "    df = df.dropna(subset=[\"lagged_market_forward_excess_returns\"])\n",
    "    return df\n",
    "\n",
    "def run_model(train_df, test_df, name):\n",
    "    train_df = make_train_with_target(train_df)\n",
    "    test_df = make_test_with_target(test_df)\n",
    "\n",
    "    num_train = train_df.select_dtypes(include=[\"number\"]).columns\n",
    "    num_test  = test_df.select_dtypes(include=[\"number\"]).columns\n",
    "    feature_cols = sorted(set(num_train).intersection(num_test) - set(cols_exclude))\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[\"lagged_market_forward_excess_returns\"]\n",
    "\n",
    "    X_test  = test_df[feature_cols]\n",
    "    y_test  = test_df[\"lagged_market_forward_excess_returns\"]\n",
    "\n",
    "    model = HistGradientBoostingRegressor(\n",
    "        max_iter=500,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    return {\n",
    "        \"method\": name,\n",
    "        \"R2\": r2_score(y_test, y_pred),\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "    }\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# 1) No imputation (raw train & raw test, model handles NaNs)\n",
    "results.append(run_model(train, test, \"no_imputation_raw\"))\n",
    "\n",
    "# 2) Backfill (train_backfill already done; just backfill test)\n",
    "test_backfill = test.copy().bfill().ffill()\n",
    "results.append(run_model(train_backfill, test_backfill, \"backfill\"))\n",
    "\n",
    "# 3) Mean (re-use train means for both train & test)\n",
    "mean_vals = train.mean(numeric_only=True)\n",
    "train_mean = train.copy().fillna(mean_vals)\n",
    "test_mean  = test.copy().fillna(mean_vals)\n",
    "results.append(run_model(train_mean, test_mean, \"mean\"))\n",
    "\n",
    "# 4) Median\n",
    "median_vals = train.median(numeric_only=True)\n",
    "train_median = train.copy().fillna(median_vals)\n",
    "test_median  = test.copy().fillna(median_vals)\n",
    "results.append(run_model(train_median, test_median, \"median\"))\n",
    "\n",
    "# 5) KNN k=2 on both train & test (same setup you used for train)\n",
    "numeric_cols = train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cols_exclude_knn = [\"date_id\", \"forward_returns\", \"risk_free_rate\",\n",
    "                    \"market_forward_excess_returns\", \"missing_indicator\"]\n",
    "cols_for_knn = [c for c in numeric_cols if c not in cols_exclude_knn]\n",
    "\n",
    "train_knn_full = train.copy()\n",
    "test_knn_full  = test.copy()\n",
    "\n",
    "X_train_knn = train_knn_full[cols_for_knn].copy()\n",
    "X_test_knn  = test_knn_full[cols_for_knn].copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_knn)\n",
    "X_test_scaled  = scaler.transform(X_test_knn)\n",
    "\n",
    "knn = KNNImputer(n_neighbors=2)\n",
    "X_train_imp_scaled = knn.fit_transform(X_train_scaled)\n",
    "X_test_imp_scaled  = knn.transform(X_test_scaled)\n",
    "\n",
    "X_train_imp = scaler.inverse_transform(X_train_imp_scaled)\n",
    "X_test_imp  = scaler.inverse_transform(X_test_imp_scaled)\n",
    "\n",
    "train_knn_full[cols_for_knn] = X_train_imp\n",
    "test_knn_full[cols_for_knn]  = X_test_imp\n",
    "\n",
    "binary_cols = [col for col in cols_for_knn if col.startswith(\"D\")]\n",
    "for col in binary_cols:\n",
    "    train_knn_full[col] = train_knn_full[col].round().astype(int)\n",
    "    test_knn_full[col]  = test_knn_full[col].round().astype(int)\n",
    "\n",
    "results.append(run_model(train_knn_full, test_knn_full, \"knn_k2\"))\n",
    "\n",
    "# Summary\n",
    "metrics_df = pd.DataFrame(results).sort_values(\"R2\", ascending=False)\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75eba7d",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "- Successfully concluded KNN-2 is the best method for dealing with missing data\n",
    "- Now selecting features, and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da363fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = train_knn_impute.copy()\n",
    "full_train[\"target\"] = full_train[\"market_forward_excess_returns\"].shift(-1)\n",
    "full_train = full_train.dropna(subset=[\"target\"])\n",
    "\n",
    "feature_cols = [c for c in full_train.columns if c not in [\n",
    "    \"date_id\",\n",
    "    \"forward_returns\",\n",
    "    \"risk_free_rate\",\n",
    "    \"market_forward_excess_returns\",\n",
    "    \"missing_indicator\",\n",
    "    \"target\",\n",
    "]]\n",
    "\n",
    "X = full_train[feature_cols]\n",
    "Y = full_train[\"target\"]\n",
    "\n",
    "X_train = full_train[full_train['missing_indicator'] == 1][feature_cols]\n",
    "Y_train = full_train[full_train['missing_indicator'] == 1][\"target\"]\n",
    "\n",
    "X_train_fit = X_train[:int(len(X_train)*0.8)]\n",
    "Y_train_fit = Y_train[:int(len(Y_train)*0.8)]\n",
    "\n",
    "X_train_val = X_train[int(len(X_train)*0.8):]\n",
    "Y_train_val = Y_train[int(len(Y_train)*0.8):]\n",
    "\n",
    "X_test = full_train[full_train['missing_indicator'] == 0][feature_cols]\n",
    "Y_test = full_train[full_train['missing_indicator'] == 0][\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f174eab",
   "metadata": {},
   "source": [
    "### First fit a LightGBM model\n",
    "- Feature importance from the  LightGBM\n",
    "- Permutation importance\n",
    "- SHAP importance\n",
    "- Pearson correlation importance\n",
    "- Mutual inmformation importance\n",
    "- Lasso coefficients importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e0235c",
   "metadata": {},
   "source": [
    "### Fitting LightGBM using X (all features)\n",
    "- Validation IC is 0.010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a73723a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001026 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001120 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001370 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001889 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001658 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001337 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001116 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001135 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002620 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001518 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001062 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001133 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001184 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001249 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001320 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001199 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000937 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000753 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000895 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001122 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001022 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000816 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001037 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000758 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000816 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000744 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000763 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000904 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000967 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000905 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001202 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001352 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000772 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001605 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001041 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000810 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000873 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000710 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000925 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000763 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000968 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001140 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000760 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001319 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001017 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001116 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000881 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001132 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002055 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000898 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000849 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000878 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000880 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011924 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000824 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000778 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000748 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000786 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000779 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001155 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000861 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000875 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000746 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000784 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001211 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21168\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "    seed    val_ic   test_ic\n",
      "0      0  0.044720  0.032851\n",
      "1      1  0.045988  0.013489\n",
      "2      2  0.068109  0.022721\n",
      "3      3  0.048813  0.028874\n",
      "4      4  0.035876  0.014794\n",
      "..   ...       ...       ...\n",
      "95    95  0.015911  0.011024\n",
      "96    96  0.059582  0.026683\n",
      "97    97  0.073097  0.031519\n",
      "98    98  0.060145  0.002777\n",
      "99    99  0.069691  0.012048\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "Validation IC mean: 0.05373789544863745 std: 0.01681536839593625\n",
      "Test IC mean: 0.02001267745174993 std: 0.009240120954251635\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import lightgbm as lgb\n",
    "\n",
    "def pearson_ic(y_true, y_pred):\n",
    "    ic = pearsonr(y_true, y_pred)[0]\n",
    "    return \"pearson_ic\", ic, True  # higher is better\n",
    "\n",
    "results = []\n",
    "\n",
    "for seed in range(100):\n",
    "    model = lgb.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.01,\n",
    "        num_leaves=31,\n",
    "        colsample_bytree=0.2,\n",
    "        subsample=0.2,\n",
    "        subsample_freq=5,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train_fit,\n",
    "        Y_train_fit,\n",
    "        eval_set=[(X_train_val, Y_train_val)],\n",
    "        eval_metric=pearson_ic,\n",
    "        # callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n",
    "    )\n",
    "\n",
    "    # Validation IC\n",
    "    y_pred_val = model.predict(X_train_val)\n",
    "    val_ic = pearsonr(y_pred_val, Y_train_val)[0]\n",
    "\n",
    "    # Test IC\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    test_ic = pearsonr(y_pred_test, Y_test)[0]\n",
    "\n",
    "    results.append({\n",
    "        \"seed\": seed,\n",
    "        \"val_ic\": val_ic,\n",
    "        \"test_ic\": test_ic,\n",
    "        # \"best_iteration\": model.best_iteration_\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(results_df)\n",
    "print(\"Validation IC mean:\", results_df[\"val_ic\"].mean(), \"std:\", results_df[\"val_ic\"].std())\n",
    "print(\"Test IC mean:\", results_df[\"test_ic\"].mean(), \"std:\", results_df[\"test_ic\"].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25d46d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giolu\\OneDrive\\Documents\\Giovanni\\Quant\\Hull Tactical\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21583\n",
      "[LightGBM] [Info] Number of data points in the train set: 8989, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.000051\n",
      "Features to drop due to correlation: ['E3', 'P10', 'I5', 'M16', 'E11', 'S10', 'D2', 'V4', 'E16']\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 1. Imports\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "import shap\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 2. Fit LightGBM model\n",
    "# ===============================\n",
    "model = lgb.LGBMRegressor(\n",
    "    objective=\"regression\",\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=31,\n",
    "    colsample_bytree=0.2,\n",
    "    subsample=0.2,\n",
    "    subsample_freq=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X,\n",
    "    Y,\n",
    "    eval_set=[(X, Y)],\n",
    "    eval_metric=\"rmse\",\n",
    "    # callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
    ")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 3. Base importance DataFrame\n",
    "# ===============================\n",
    "importance_df = pd.DataFrame(index=feature_cols)\n",
    "\n",
    "booster = model.booster_\n",
    "\n",
    "# LightGBM built-in importance\n",
    "lgb_split_importance = booster.feature_importance(importance_type='split')\n",
    "lgb_gain_importance = booster.feature_importance(importance_type='gain')\n",
    "\n",
    "importance_df[\"lgbm_split_importance\"] = lgb_split_importance\n",
    "importance_df[\"lgbm_gain_importance\"] = lgb_gain_importance\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 4. Permutation importance\n",
    "# ===============================\n",
    "perm_result = permutation_importance(\n",
    "    model,\n",
    "    X,\n",
    "    Y,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "importance_df[\"perm_importance_mean\"] = perm_result.importances_mean\n",
    "# importance_df[\"perm_importance_std\"] = perm_result.importances_std  # keep if you want diagnostics\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 5. SHAP, Mutual Information, Pearson with target\n",
    "# ===============================\n",
    "\n",
    "# --- SHAP (mean |SHAP|) ---\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "importance_df[\"shap_mean_abs\"] = mean_abs_shap\n",
    "\n",
    "# --- Mutual Information with target ---\n",
    "mi = mutual_info_regression(\n",
    "    X,\n",
    "    Y,\n",
    "    random_state=42\n",
    ")\n",
    "importance_df[\"mutual_information\"] = mi\n",
    "\n",
    "# --- Pearson correlation with target ---\n",
    "pearson_with_target = X.corrwith(Y)\n",
    "importance_df[\"pearson_with_target\"] = pearson_with_target.values\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 6. Lasso feature importance\n",
    "# ===============================\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# alpha controls sparsity; adjust as needed\n",
    "lasso = Lasso(alpha=0.00001, random_state=42)\n",
    "lasso.fit(X_scaled, Y)\n",
    "\n",
    "lasso_importance = np.abs(lasso.coef_)\n",
    "importance_df[\"lasso_coef_abs\"] = lasso_importance\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 7. Composite score from normalised metrics\n",
    "# ===============================\n",
    "\n",
    "# Columns to use (drop split importance due to bias)\n",
    "cols_to_use = [\n",
    "    \"lgbm_gain_importance\",\n",
    "    \"perm_importance_mean\",\n",
    "    \"shap_mean_abs\",\n",
    "    \"mutual_information\",\n",
    "    \"pearson_with_target\",\n",
    "    \"lasso_coef_abs\"\n",
    "]\n",
    "\n",
    "# --- Normalise each metric (z-scores) ---\n",
    "normalized = (importance_df[cols_to_use] - importance_df[cols_to_use].mean()) / importance_df[cols_to_use].std()\n",
    "\n",
    "# --- Define weights ---\n",
    "weights = {\n",
    "    \"shap_mean_abs\": 0.30,\n",
    "    \"perm_importance_mean\": 0.30,\n",
    "    \"lgbm_gain_importance\": 0.15,\n",
    "    \"mutual_information\": 0.10,\n",
    "    \"pearson_with_target\": 0.10,\n",
    "    \"lasso_coef_abs\": 0.05\n",
    "}\n",
    "\n",
    "# --- Composite score ---\n",
    "importance_df[\"composite_score\"] = sum(\n",
    "    normalized[col] * weights[col] for col in cols_to_use\n",
    ")\n",
    "\n",
    "# --- Rank features ---\n",
    "importance_df[\"rank\"] = importance_df[\"composite_score\"].rank(ascending=False)\n",
    "\n",
    "# --- Final ranked table ---\n",
    "ranked_features = importance_df.sort_values(\"composite_score\", ascending=False)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 8. Correlation-based pruning (keep best by composite score)\n",
    "# ===============================\n",
    "corr = X.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "\n",
    "to_drop = []\n",
    "\n",
    "for col in upper.columns:\n",
    "    high_corr = upper[col][upper[col] > 0.90]  # correlation threshold\n",
    "    if len(high_corr) > 0:\n",
    "        correlated_feats = [col] + list(high_corr.index)\n",
    "\n",
    "        # Keep the best feature by composite score\n",
    "        best = importance_df.loc[correlated_feats][\"composite_score\"].idxmax()\n",
    "\n",
    "        # Drop all *except* the best\n",
    "        for f in correlated_feats:\n",
    "            if f != best:\n",
    "                to_drop.append(f)\n",
    "\n",
    "to_drop = list(set(to_drop))\n",
    "\n",
    "print(\"Features to drop due to correlation:\", to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b89fed0",
   "metadata": {},
   "source": [
    "- We have ran importance checks on all features\n",
    "- Removed features with pearson corr over 0.9 by importance\n",
    "- Now repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45a60825",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = X.drop(columns=to_drop)\n",
    "\n",
    "feature_cols_1 = X_1.columns.tolist()\n",
    "\n",
    "X_1_train = X_train.drop(columns=to_drop)\n",
    "X_1_test  = X_test.drop(columns=to_drop)\n",
    "\n",
    "X_1_train_fit = X_1_train[:int(len(X_1_train)*0.8)]\n",
    "X_1_train_val = X_1_train[int(len(X_1_train)*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb48025f",
   "metadata": {},
   "source": [
    "### Fitting LightGBM using X_1\n",
    "- Validation IC is 0.034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb136da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001113 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001543 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001177 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000828 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001099 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001057 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000822 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000857 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000959 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000770 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000737 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000774 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000790 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000802 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000771 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000779 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000750 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000762 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000981 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001141 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000870 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000865 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000883 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000967 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000951 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001090 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000801 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000767 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000728 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000841 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000715 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000706 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000699 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000799 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000780 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000735 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000699 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000810 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000698 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000707 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000843 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000696 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000726 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000447 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000761 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000845 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000744 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000983 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000922 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000927 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001171 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000801 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000907 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000931 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000912 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000918 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000903 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000958 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000923 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000820 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000836 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000957 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000796 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000682 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000910 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000673 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000755 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000726 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000775 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000778 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000918 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000806 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000725 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000690 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000740 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000686 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000796 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000831 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000821 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19357\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "    seed    val_ic   test_ic\n",
      "0      0  0.016453  0.020411\n",
      "1      1  0.057461  0.010961\n",
      "2      2  0.049419  0.041616\n",
      "3      3  0.038689  0.027302\n",
      "4      4  0.042105  0.024131\n",
      "..   ...       ...       ...\n",
      "95    95  0.029975  0.009598\n",
      "96    96  0.072648  0.032480\n",
      "97    97  0.067846  0.042881\n",
      "98    98  0.080079  0.024319\n",
      "99    99  0.042564  0.017904\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "Validation IC mean: 0.05009211813893783 std: 0.018491110797659978\n",
      "Test IC mean: 0.019305644126676413 std: 0.011006122458853147\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def pearson_ic(y_true, y_pred):\n",
    "    ic = pearsonr(y_true, y_pred)[0]\n",
    "    return \"pearson_ic\", ic, True  # higher is better\n",
    "\n",
    "results = []\n",
    "\n",
    "for seed in range(100):\n",
    "    model = lgb.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.01,\n",
    "        num_leaves=31,\n",
    "        colsample_bytree=0.2,\n",
    "        subsample=0.2,\n",
    "        subsample_freq=5,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_1_train_fit,\n",
    "        Y_train_fit,\n",
    "        eval_set=[(X_1_train_val, Y_train_val)],\n",
    "        eval_metric=pearson_ic,\n",
    "        # callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n",
    "    )\n",
    "\n",
    "    # Validation IC\n",
    "    y_pred_val = model.predict(X_1_train_val)\n",
    "    val_ic = pearsonr(y_pred_val, Y_train_val)[0]\n",
    "\n",
    "    # Test IC\n",
    "    y_pred_test = model.predict(X_1_test)\n",
    "    test_ic = pearsonr(y_pred_test, Y_test)[0]\n",
    "\n",
    "    results.append({\n",
    "        \"seed\": seed,\n",
    "        \"val_ic\": val_ic,\n",
    "        \"test_ic\": test_ic,\n",
    "        # \"best_iteration\": model.best_iteration_\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(results_df)\n",
    "print(\"Validation IC mean:\", results_df[\"val_ic\"].mean(), \"std:\", results_df[\"val_ic\"].std())\n",
    "print(\"Test IC mean:\", results_df[\"test_ic\"].mean(), \"std:\", results_df[\"test_ic\"].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "365b84dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19542\n",
      "[LightGBM] [Info] Number of data points in the train set: 8989, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score 0.000051\n",
      "Features to drop due to correlation: ['P11', 'V2', 'V7', 'P2', 'S4', 'I4']\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 1. Imports\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "import shap\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 2. Fit LightGBM model\n",
    "# ===============================\n",
    "model = lgb.LGBMRegressor(\n",
    "    objective=\"regression\",\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=31,\n",
    "    colsample_bytree=0.2,\n",
    "    subsample=0.2,\n",
    "    subsample_freq=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_1,\n",
    "    Y,\n",
    "    eval_set=[(X_1, Y)],\n",
    "    eval_metric=\"rmse\",\n",
    "    # callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
    ")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 3. Base importance DataFrame\n",
    "# ===============================\n",
    "importance_df = pd.DataFrame(index=feature_cols_1)\n",
    "\n",
    "booster = model.booster_\n",
    "\n",
    "# LightGBM built-in importance\n",
    "lgb_split_importance = booster.feature_importance(importance_type='split')\n",
    "lgb_gain_importance = booster.feature_importance(importance_type='gain')\n",
    "\n",
    "importance_df[\"lgbm_split_importance\"] = lgb_split_importance\n",
    "importance_df[\"lgbm_gain_importance\"] = lgb_gain_importance\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 4. Permutation importance\n",
    "# ===============================\n",
    "perm_result = permutation_importance(\n",
    "    model,\n",
    "    X_1,\n",
    "    Y,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "importance_df[\"perm_importance_mean\"] = perm_result.importances_mean\n",
    "# importance_df[\"perm_importance_std\"] = perm_result.importances_std  # keep if you want diagnostics\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 5. SHAP, Mutual Information, Pearson with target\n",
    "# ===============================\n",
    "\n",
    "# --- SHAP (mean |SHAP|) ---\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_1)\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "importance_df[\"shap_mean_abs\"] = mean_abs_shap\n",
    "\n",
    "# --- Mutual Information with target ---\n",
    "mi = mutual_info_regression(\n",
    "    X_1,\n",
    "    Y,\n",
    "    random_state=42\n",
    ")\n",
    "importance_df[\"mutual_information\"] = mi\n",
    "\n",
    "# --- Pearson correlation with target ---\n",
    "pearson_with_target = X_1.corrwith(Y)\n",
    "importance_df[\"pearson_with_target\"] = pearson_with_target.values\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 6. Lasso feature importance\n",
    "# ===============================\n",
    "scaler = StandardScaler()\n",
    "X_1_scaled = scaler.fit_transform(X_1)\n",
    "\n",
    "# alpha controls sparsity; adjust as needed\n",
    "lasso = Lasso(alpha=0.00001, random_state=42)\n",
    "lasso.fit(X_1_scaled, Y)\n",
    "\n",
    "lasso_importance = np.abs(lasso.coef_)\n",
    "importance_df[\"lasso_coef_abs\"] = lasso_importance\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 7. Composite score from normalised metrics\n",
    "# ===============================\n",
    "\n",
    "# Columns to use (drop split importance due to bias)\n",
    "cols_to_use = [\n",
    "    \"lgbm_gain_importance\",\n",
    "    \"perm_importance_mean\",\n",
    "    \"shap_mean_abs\",\n",
    "    \"mutual_information\",\n",
    "    \"pearson_with_target\",\n",
    "    \"lasso_coef_abs\"\n",
    "]\n",
    "\n",
    "# --- Normalise each metric (z-scores) ---\n",
    "normalized = (importance_df[cols_to_use] - importance_df[cols_to_use].mean()) / importance_df[cols_to_use].std()\n",
    "\n",
    "# --- Define weights ---\n",
    "weights = {\n",
    "    \"shap_mean_abs\": 0.30,\n",
    "    \"perm_importance_mean\": 0.30,\n",
    "    \"lgbm_gain_importance\": 0.15,\n",
    "    \"mutual_information\": 0.10,\n",
    "    \"pearson_with_target\": 0.10,\n",
    "    \"lasso_coef_abs\": 0.05\n",
    "}\n",
    "\n",
    "# --- Composite score ---\n",
    "importance_df[\"composite_score\"] = sum(\n",
    "    normalized[col] * weights[col] for col in cols_to_use\n",
    ")\n",
    "\n",
    "# --- Rank features ---\n",
    "importance_df[\"rank\"] = importance_df[\"composite_score\"].rank(ascending=False)\n",
    "\n",
    "# --- Final ranked table ---\n",
    "ranked_features = importance_df.sort_values(\"composite_score\", ascending=False)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 8. Correlation-based pruning (keep best by composite score)\n",
    "# ===============================\n",
    "corr = X_1.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "\n",
    "to_drop = []\n",
    "\n",
    "for col in upper.columns:\n",
    "    high_corr = upper[col][upper[col] > 0.8]  # correlation threshold\n",
    "    if len(high_corr) > 0:\n",
    "        correlated_feats = [col] + list(high_corr.index)\n",
    "\n",
    "        # Keep the best feature by composite score\n",
    "        best = importance_df.loc[correlated_feats][\"composite_score\"].idxmax()\n",
    "\n",
    "        # Drop all *except* the best\n",
    "        for f in correlated_feats:\n",
    "            if f != best:\n",
    "                to_drop.append(f)\n",
    "\n",
    "to_drop = list(set(to_drop))\n",
    "\n",
    "print(\"Features to drop due to correlation:\", to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7c3646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = X_1.drop(columns=to_drop)\n",
    "\n",
    "feature_cols_2 = X_2.columns.tolist()\n",
    "\n",
    "X_2_train = X_1_train.drop(columns=to_drop)\n",
    "X_2_test  = X_1_test.drop(columns=to_drop)\n",
    "\n",
    "X_2_train_fit = X_2_train[:int(len(X_2_train)*0.8)]\n",
    "X_2_train_val = X_2_train[int(len(X_2_train)*0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc315c",
   "metadata": {},
   "source": [
    "### Fitting LightGBM using X_2\n",
    "- Validation IC is 0.0268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b633f55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000762 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001093 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001775 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001826 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000930 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000870 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000881 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000849 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000795 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000877 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000908 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000890 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000608 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000843 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000763 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000837 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000865 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001361 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000856 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000906 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000830 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000795 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000810 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001376 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001129 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000640 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000665 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000786 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000743 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000655 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000755 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000762 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000798 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000686 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000798 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000779 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000818 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000725 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001000 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000782 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000826 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000813 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000756 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000678 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000714 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000689 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000746 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000778 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000770 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000770 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000749 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000775 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000685 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000847 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000789 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001104 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000825 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000753 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000807 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000933 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000780 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000941 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000955 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000953 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001047 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000746 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000826 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000823 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000785 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000770 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000702 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000679 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17831\n",
      "[LightGBM] [Info] Number of data points in the train set: 5575, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score -0.000053\n",
      "    seed    val_ic   test_ic\n",
      "0      0  0.054371  0.018513\n",
      "1      1  0.062767  0.009448\n",
      "2      2  0.064869  0.028924\n",
      "3      3  0.022330  0.024481\n",
      "4      4  0.047677  0.002917\n",
      "..   ...       ...       ...\n",
      "95    95  0.049704  0.020609\n",
      "96    96  0.025778  0.031237\n",
      "97    97  0.051124  0.009763\n",
      "98    98  0.057420  0.005094\n",
      "99    99  0.049565  0.030787\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "Validation IC mean: 0.04878410413912728 std: 0.018964856958843205\n",
      "Test IC mean: 0.017933077525676907 std: 0.011880408771220612\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def pearson_ic(y_true, y_pred):\n",
    "    ic = pearsonr(y_true, y_pred)[0]\n",
    "    return \"pearson_ic\", ic, True  # higher is better\n",
    "\n",
    "results = []\n",
    "\n",
    "for seed in range(100):\n",
    "    model = lgb.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.01,\n",
    "        num_leaves=31,\n",
    "        colsample_bytree=0.2,\n",
    "        subsample=0.2,\n",
    "        subsample_freq=5,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_2_train_fit,\n",
    "        Y_train_fit,\n",
    "        eval_set=[(X_2_train_val, Y_train_val)],\n",
    "        eval_metric=pearson_ic,\n",
    "        # callbacks=[lgb.early_stopping(stopping_rounds=200, verbose=False)]\n",
    "    )\n",
    "\n",
    "    # Validation IC\n",
    "    y_pred_val = model.predict(X_2_train_val)\n",
    "    val_ic = pearsonr(y_pred_val, Y_train_val)[0]\n",
    "\n",
    "    # Test IC\n",
    "    y_pred_test = model.predict(X_2_test)\n",
    "    test_ic = pearsonr(y_pred_test, Y_test)[0]\n",
    "\n",
    "    results.append({\n",
    "        \"seed\": seed,\n",
    "        \"val_ic\": val_ic,\n",
    "        \"test_ic\": test_ic,\n",
    "        # \"best_iteration\": model.best_iteration_\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(results_df)\n",
    "print(\"Validation IC mean:\", results_df[\"val_ic\"].mean(), \"std:\", results_df[\"val_ic\"].std())\n",
    "print(\"Test IC mean:\", results_df[\"test_ic\"].mean(), \"std:\", results_df[\"test_ic\"].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2a5d0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001632 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18016\n",
      "[LightGBM] [Info] Number of data points in the train set: 8989, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score 0.000051\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lgbm_split_importance",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "lgbm_gain_importance",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "perm_importance_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "shap_mean_abs",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mutual_information",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pearson_with_target",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lasso_coef_abs",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "composite_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rank",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "4b496705-b373-486d-b110-6844c7c44e5d",
       "rows": [
        [
         "M4",
         "926",
         "0.8041870121087413",
         "0.032934867164547764",
         "0.00038128549034488853",
         "0.04985628430579325",
         "-0.030509390425405937",
         "0.0003745351344481277",
         "3.96846506004738",
         "1.0"
        ],
        [
         "S8",
         "685",
         "0.603584414900979",
         "0.02073371608902531",
         "0.00020287214804184218",
         "0.08365566705780658",
         "-0.01130333871569245",
         "0.00031286129755361846",
         "2.3377287883835827",
         "2.0"
        ],
        [
         "V13",
         "449",
         "0.3856098205142189",
         "0.015328166644584296",
         "0.00016672129495519196",
         "0.0658934893208567",
         "0.011176947963608339",
         "0.0003923652193379377",
         "1.67106239143755",
         "3.0"
        ],
        [
         "V9",
         "399",
         "0.3805064173066057",
         "0.012762337022673643",
         "0.00015583356760210063",
         "0.08405490401342242",
         "0.01245180590499828",
         "0.00041509259821816965",
         "1.549697051509643",
         "4.0"
        ],
        [
         "M12",
         "336",
         "0.2574381371960044",
         "0.008750520713957622",
         "0.00022645562289776969",
         "0.03136202248815767",
         "-0.024567206627786622",
         "0.0007021255199522756",
         "1.086752174617872",
         "5.0"
        ],
        [
         "S3",
         "352",
         "0.2852247742994223",
         "0.01212454958287763",
         "0.00019362679456679823",
         "0.01782641817121977",
         "-0.022518345309152615",
         "0.00021797464740390838",
         "0.9968213108200993",
         "6.0"
        ],
        [
         "S1",
         "509",
         "0.38331299569108523",
         "0.01160342413099481",
         "0.00010254852933899285",
         "0.025128398185358947",
         "0.00828221487978837",
         "0.0004406056713319207",
         "0.9043372634473863",
         "7.0"
        ],
        [
         "I3",
         "437",
         "0.31853385939029977",
         "0.010029537951601119",
         "0.00011155537031190715",
         "0.04608938708502208",
         "0.010420496024506704",
         "0.00015170978949104384",
         "0.8317654188737679",
         "8.0"
        ],
        [
         "V5",
         "429",
         "0.32542363164247945",
         "0.011930549070605135",
         "0.00013313574396748643",
         "0.017782912118903482",
         "-0.0026923855473291345",
         "7.42658122053451e-05",
         "0.8254999760354887",
         "9.0"
        ],
        [
         "P6",
         "409",
         "0.340868905899697",
         "0.010796003448994718",
         "0.00010844115102430732",
         "0.0333695239090277",
         "0.006067881002013877",
         "0.00018992560306356715",
         "0.8050074773341611",
         "10.0"
        ],
        [
         "E19",
         "230",
         "0.20463549194391817",
         "0.0072115196750779376",
         "7.727086447776347e-05",
         "0.10570135775648115",
         "0.01171840832268625",
         "9.042755236396747e-05",
         "0.6192245990109417",
         "11.0"
        ],
        [
         "V10",
         "209",
         "0.18209043631213717",
         "0.007552792870154912",
         "9.058422763387959e-05",
         "0.0721787747565541",
         "0.01864418485122252",
         "0.00022731604456355047",
         "0.6116469085486778",
         "12.0"
        ],
        [
         "M17",
         "230",
         "0.17968853609636426",
         "0.005962686665180972",
         "0.00010678087063343907",
         "0.05863426796596105",
         "0.02544464758662262",
         "8.604188900053983e-05",
         "0.5554678174759194",
         "13.0"
        ],
        [
         "S5",
         "169",
         "0.13997850677696988",
         "0.005141936095775135",
         "0.00011394108805484946",
         "0.03488902759949575",
         "0.038519415078783134",
         "0.00029888942390657005",
         "0.5320744200752798",
         "14.0"
        ],
        [
         "P13",
         "375",
         "0.2753956348169595",
         "0.009170431697580795",
         "0.00011608333469326103",
         "0.008247427743132363",
         "-0.0010782318461866147",
         "9.91644917842914e-05",
         "0.48410061165682816",
         "15.0"
        ],
        [
         "S6",
         "262",
         "0.21005073183914647",
         "0.0070527977309223664",
         "0.00010802756695675796",
         "0.015464337778499981",
         "0.0179969012578686",
         "0.00025672610953108877",
         "0.44668825735031553",
         "16.0"
        ],
        [
         "E2",
         "244",
         "0.21202203319990076",
         "0.007210145156745396",
         "9.502222912212317e-05",
         "0.05084376002000912",
         "0.00636715198389768",
         "0.0001636124317290514",
         "0.4459646816852836",
         "17.0"
        ],
        [
         "E8",
         "286",
         "0.24737822273164056",
         "0.006616502399104307",
         "0.00010396662374687399",
         "0.03392638433040496",
         "0.00553777215069527",
         "0.000198019582628141",
         "0.42134730242221874",
         "18.0"
        ],
        [
         "E13",
         "422",
         "0.3046534011082258",
         "0.008196791977006734",
         "0.00011926160538222533",
         "0.009702173691608529",
         "-0.009200280294223361",
         "0.0",
         "0.40037505071221646",
         "19.0"
        ],
        [
         "M2",
         "312",
         "0.24551947772852145",
         "0.00783649621016561",
         "8.134583669253448e-05",
         "0.022687752170617692",
         "0.014825693535570016",
         "9.7704235900657e-05",
         "0.36854786336232864",
         "20.0"
        ],
        [
         "I9",
         "249",
         "0.19503859165706672",
         "0.006768500691399737",
         "9.596228159902528e-05",
         "0.04976773023360881",
         "0.004817764560278401",
         "0.0",
         "0.352735307599057",
         "21.0"
        ],
        [
         "E4",
         "283",
         "0.21136076503898948",
         "0.0077259122953755475",
         "0.00010188985698283734",
         "0.0",
         "0.007625789603680194",
         "4.528657911336492e-05",
         "0.2628054281335979",
         "22.0"
        ],
        [
         "E18",
         "290",
         "0.21692752580565866",
         "0.006159780168104245",
         "7.388946053245693e-05",
         "0.048045881886566555",
         "-0.009501928419218266",
         "0.00038377773264196437",
         "0.19375336951695354",
         "23.0"
        ],
        [
         "P8",
         "250",
         "0.21177021486801095",
         "0.006701904845799611",
         "8.449456269358644e-05",
         "0.04431754477304217",
         "-0.017390664446230306",
         "5.3791574334384034e-05",
         "0.1296733070458537",
         "24.0"
        ],
        [
         "M10",
         "186",
         "0.15189833103795536",
         "0.005413235441152298",
         "7.029295596925713e-05",
         "0.03148986958096245",
         "0.012385735012457214",
         "0.00015699293239008395",
         "0.09136907340614556",
         "25.0"
        ],
        [
         "M6",
         "187",
         "0.14126474285149015",
         "0.004771379622607252",
         "7.405568942946294e-05",
         "0.03321293299662109",
         "0.016564311397397842",
         "0.00010337999685352333",
         "0.08732345758000415",
         "26.0"
        ],
        [
         "M8",
         "296",
         "0.2087355774419848",
         "0.007441043190593144",
         "7.607363046784231e-05",
         "0.016013681749588393",
         "-0.014422559323699175",
         "8.33191629162283e-05",
         "0.02475416989936701",
         "27.0"
        ],
        [
         "M11",
         "113",
         "0.10029485047562048",
         "0.0024328303450184994",
         "5.04474786205596e-05",
         "0.037890829878922005",
         "0.00653003158745136",
         "0.0013055686220562394",
         "-0.017275294519850187",
         "28.0"
        ],
        [
         "P5",
         "158",
         "0.12539147562347353",
         "0.0045889051972534565",
         "8.345272580499031e-05",
         "0.058480664103103486",
         "-0.015241642836068766",
         "4.6119050926956517e-05",
         "-0.023427899364335654",
         "29.0"
        ],
        [
         "M1",
         "116",
         "0.10071634102496319",
         "0.002764129019028705",
         "4.0931147495359044e-05",
         "0.06548445888990706",
         "0.01580762854993684",
         "0.00041518570552405676",
         "-0.04566101352092717",
         "30.0"
        ],
        [
         "M14",
         "201",
         "0.16271008331386838",
         "0.004652838173110374",
         "7.240717803599351e-05",
         "0.014559946361313969",
         "0.011570437570544646",
         "0.0",
         "-0.04921076456919306",
         "31.0"
        ],
        [
         "I7",
         "182",
         "0.13883076296770014",
         "0.004130496901276548",
         "5.7243969197929256e-05",
         "0.01676608495577092",
         "0.00912756666719799",
         "0.0006713104659063837",
         "-0.05023493794111421",
         "32.0"
        ],
        [
         "S2",
         "164",
         "0.13685283993254416",
         "0.005250710745249942",
         "9.752278666645236e-05",
         "0.03827950250845813",
         "-0.02996021011307403",
         "0.00025835992686168956",
         "-0.05098324650207989",
         "33.0"
        ],
        [
         "I1",
         "183",
         "0.12830656705773436",
         "0.004151475739496579",
         "7.408815577905704e-05",
         "0.025069569637246936",
         "0.0013602385174625138",
         "0.00030270125485611836",
         "-0.07124560637001541",
         "34.0"
        ],
        [
         "E6",
         "171",
         "0.12463771580951288",
         "0.0038813049751738383",
         "4.4571163726462e-05",
         "0.04215057680234846",
         "0.01558150737178655",
         "0.00023916550275719555",
         "-0.07848355937709968",
         "35.0"
        ],
        [
         "P4",
         "193",
         "0.14334781319485046",
         "0.00639347563407755",
         "6.906687748607168e-05",
         "0.004179438018206305",
         "-0.0014450505180577504",
         "1.0629879978849967e-05",
         "-0.12494043986445884",
         "36.0"
        ],
        [
         "D5",
         "41",
         "0.027208533021621406",
         "0.0021054668360952823",
         "9.051531955283542e-05",
         "0.005694207726224976",
         "0.022998630035512623",
         "0.00038900812467300915",
         "-0.13422103872400415",
         "37.0"
        ],
        [
         "E17",
         "121",
         "0.08632067058351822",
         "0.0019375010278315896",
         "3.9658251604260374e-05",
         "0.056630315891573524",
         "0.008644647491564409",
         "0.0006874087904758302",
         "-0.15332010988433825",
         "38.0"
        ],
        [
         "M13",
         "120",
         "0.09754461355623789",
         "0.003253745974961875",
         "5.75116946949663e-05",
         "0.0558530894817153",
         "-0.0019560962698674873",
         "0.00011903995824217",
         "-0.17170764129216354",
         "39.0"
        ],
        [
         "V3",
         "184",
         "0.13113118102774024",
         "0.00472140976975508",
         "6.045077065330049e-05",
         "0.00929160922741179",
         "-0.0013311851822119052",
         "0.0004092655864031998",
         "-0.17485226345879404",
         "40.0"
        ],
        [
         "E15",
         "166",
         "0.12418750341748819",
         "0.0034905436013893286",
         "5.465081247985724e-05",
         "0.03882148250025175",
         "0.0028437262677062764",
         "0.0001175485001789454",
         "-0.1851855344573855",
         "41.0"
        ],
        [
         "E10",
         "170",
         "0.12113541012513451",
         "0.0037896520232058584",
         "5.312290973509842e-05",
         "0.0442439599738087",
         "0.0018480465822744163",
         "0.0",
         "-0.18665419824624957",
         "42.0"
        ],
        [
         "P7",
         "196",
         "0.15445595124037936",
         "0.00542634325276945",
         "5.468172890184798e-05",
         "0.028640549300048335",
         "-0.014696538971758462",
         "0.00011195752344617697",
         "-0.21056841865745501",
         "43.0"
        ],
        [
         "E9",
         "80",
         "0.0560398998495657",
         "0.001594258791435732",
         "3.729303124690396e-05",
         "0.04624122716078283",
         "0.009154889207209718",
         "0.0007579393608449986",
         "-0.24986815984907623",
         "44.0"
        ],
        [
         "S12",
         "190",
         "0.14306399269844405",
         "0.0045761314053954",
         "6.188871039299043e-05",
         "0.0",
         "-0.0009239490637370709",
         "0.00014191065201977707",
         "-0.260318038948707",
         "45.0"
        ],
        [
         "P1",
         "156",
         "0.11732095733168535",
         "0.003153471744703218",
         "5.0139720159483854e-05",
         "0.03146840199369727",
         "-0.0011081914940858355",
         "0.0002238563217706252",
         "-0.2772055484044215",
         "46.0"
        ],
        [
         "E20",
         "84",
         "0.06489377730758861",
         "0.0021631875164483615",
         "5.427735317452583e-05",
         "0.059465950073252394",
         "-0.004236597727495243",
         "3.8104790996996005e-05",
         "-0.31017021813498014",
         "47.0"
        ],
        [
         "M3",
         "128",
         "0.0964045139553491",
         "0.003163697773030205",
         "4.200659454995722e-05",
         "0.027670980042085347",
         "0.005723246441523841",
         "0.00012532794400921607",
         "-0.33199538383614896",
         "48.0"
        ],
        [
         "P9",
         "107",
         "0.08419299297383986",
         "0.002341631818872647",
         "4.0428890349577736e-05",
         "0.04091854443900367",
         "0.004952866083039741",
         "0.00020231360831080857",
         "-0.3322369869982871",
         "49.0"
        ],
        [
         "I8",
         "95",
         "0.06635282715433277",
         "0.0025676148702699075",
         "5.3335886370376676e-05",
         "0.03178537038083995",
         "-1.608519838623501e-05",
         "0.0002695644007963352",
         "-0.3345591267919455",
         "50.0"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 79
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgbm_split_importance</th>\n",
       "      <th>lgbm_gain_importance</th>\n",
       "      <th>perm_importance_mean</th>\n",
       "      <th>shap_mean_abs</th>\n",
       "      <th>mutual_information</th>\n",
       "      <th>pearson_with_target</th>\n",
       "      <th>lasso_coef_abs</th>\n",
       "      <th>composite_score</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>M4</th>\n",
       "      <td>926</td>\n",
       "      <td>0.804187</td>\n",
       "      <td>0.032935</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.049856</td>\n",
       "      <td>-0.030509</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>3.968465</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S8</th>\n",
       "      <td>685</td>\n",
       "      <td>0.603584</td>\n",
       "      <td>0.020734</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.083656</td>\n",
       "      <td>-0.011303</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>2.337729</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V13</th>\n",
       "      <td>449</td>\n",
       "      <td>0.385610</td>\n",
       "      <td>0.015328</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.065893</td>\n",
       "      <td>0.011177</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>1.671062</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V9</th>\n",
       "      <td>399</td>\n",
       "      <td>0.380506</td>\n",
       "      <td>0.012762</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.084055</td>\n",
       "      <td>0.012452</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>1.549697</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M12</th>\n",
       "      <td>336</td>\n",
       "      <td>0.257438</td>\n",
       "      <td>0.008751</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.031362</td>\n",
       "      <td>-0.024567</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>1.086752</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.002934</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014214</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>-0.882788</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D9</th>\n",
       "      <td>12</td>\n",
       "      <td>0.008136</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.002278</td>\n",
       "      <td>-0.006327</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>-0.951390</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D6</th>\n",
       "      <td>10</td>\n",
       "      <td>0.008201</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>-0.004725</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-1.018640</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.003531</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.002525</td>\n",
       "      <td>-0.010261</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>-1.053088</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D7</th>\n",
       "      <td>4</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.011098</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>-1.070007</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lgbm_split_importance  lgbm_gain_importance  perm_importance_mean  \\\n",
       "M4                     926              0.804187              0.032935   \n",
       "S8                     685              0.603584              0.020734   \n",
       "V13                    449              0.385610              0.015328   \n",
       "V9                     399              0.380506              0.012762   \n",
       "M12                    336              0.257438              0.008751   \n",
       "..                     ...                   ...                   ...   \n",
       "D3                       5              0.002934              0.000112   \n",
       "D9                      12              0.008136              0.000354   \n",
       "D6                      10              0.008201              0.000247   \n",
       "D1                       4              0.003531              0.000121   \n",
       "D7                       4              0.002458              0.000119   \n",
       "\n",
       "     shap_mean_abs  mutual_information  pearson_with_target  lasso_coef_abs  \\\n",
       "M4        0.000381            0.049856            -0.030509        0.000375   \n",
       "S8        0.000203            0.083656            -0.011303        0.000313   \n",
       "V13       0.000167            0.065893             0.011177        0.000392   \n",
       "V9        0.000156            0.084055             0.012452        0.000415   \n",
       "M12       0.000226            0.031362            -0.024567        0.000702   \n",
       "..             ...                 ...                  ...             ...   \n",
       "D3        0.000004            0.000000             0.014214        0.000160   \n",
       "D9        0.000013            0.002278            -0.006327        0.000180   \n",
       "D6        0.000006            0.001531            -0.004725        0.000022   \n",
       "D1        0.000006            0.002525            -0.010261        0.000104   \n",
       "D7        0.000006            0.000000            -0.011098        0.000115   \n",
       "\n",
       "     composite_score  rank  \n",
       "M4          3.968465   1.0  \n",
       "S8          2.337729   2.0  \n",
       "V13         1.671062   3.0  \n",
       "V9          1.549697   4.0  \n",
       "M12         1.086752   5.0  \n",
       "..               ...   ...  \n",
       "D3         -0.882788  75.0  \n",
       "D9         -0.951390  76.0  \n",
       "D6         -1.018640  77.0  \n",
       "D1         -1.053088  78.0  \n",
       "D7         -1.070007  79.0  \n",
       "\n",
       "[79 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================\n",
    "# 1. Imports\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "import shap\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 2. Fit LightGBM model\n",
    "# ===============================\n",
    "model = lgb.LGBMRegressor(\n",
    "    objective=\"regression\",\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=31,\n",
    "    colsample_bytree=0.2,\n",
    "    subsample=0.2,\n",
    "    subsample_freq=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_2,\n",
    "    Y,\n",
    "    eval_set=[(X_2, Y)],\n",
    "    eval_metric=\"rmse\",\n",
    "    # callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
    ")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 3. Base importance DataFrame\n",
    "# ===============================\n",
    "importance_df = pd.DataFrame(index=feature_cols_2)\n",
    "\n",
    "booster = model.booster_\n",
    "\n",
    "# LightGBM built-in importance\n",
    "lgb_split_importance = booster.feature_importance(importance_type='split')\n",
    "lgb_gain_importance = booster.feature_importance(importance_type='gain')\n",
    "\n",
    "importance_df[\"lgbm_split_importance\"] = lgb_split_importance\n",
    "importance_df[\"lgbm_gain_importance\"] = lgb_gain_importance\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 4. Permutation importance\n",
    "# ===============================\n",
    "perm_result = permutation_importance(\n",
    "    model,\n",
    "    X_2,\n",
    "    Y,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "importance_df[\"perm_importance_mean\"] = perm_result.importances_mean\n",
    "# importance_df[\"perm_importance_std\"] = perm_result.importances_std  # keep if you want diagnostics\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 5. SHAP, Mutual Information, Pearson with target\n",
    "# ===============================\n",
    "\n",
    "# --- SHAP (mean |SHAP|) ---\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_2)\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "importance_df[\"shap_mean_abs\"] = mean_abs_shap\n",
    "\n",
    "# --- Mutual Information with target ---\n",
    "mi = mutual_info_regression(\n",
    "    X_2,\n",
    "    Y,\n",
    "    random_state=42\n",
    ")\n",
    "importance_df[\"mutual_information\"] = mi\n",
    "\n",
    "# --- Pearson correlation with target ---\n",
    "pearson_with_target = X_2.corrwith(Y)\n",
    "importance_df[\"pearson_with_target\"] = pearson_with_target.values\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 6. Lasso feature importance\n",
    "# ===============================\n",
    "scaler = StandardScaler()\n",
    "X_2_scaled = scaler.fit_transform(X_2)\n",
    "\n",
    "# alpha controls sparsity; adjust as needed\n",
    "lasso = Lasso(alpha=0.00001, random_state=42)\n",
    "lasso.fit(X_2_scaled, Y)\n",
    "\n",
    "lasso_importance = np.abs(lasso.coef_)\n",
    "importance_df[\"lasso_coef_abs\"] = lasso_importance\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 7. Composite score from normalised metrics\n",
    "# ===============================\n",
    "\n",
    "# Columns to use (drop split importance due to bias)\n",
    "cols_to_use = [\n",
    "    \"lgbm_gain_importance\",\n",
    "    \"perm_importance_mean\",\n",
    "    \"shap_mean_abs\",\n",
    "    \"mutual_information\",\n",
    "    \"pearson_with_target\",\n",
    "    \"lasso_coef_abs\"\n",
    "]\n",
    "\n",
    "# --- Normalise each metric (z-scores) ---\n",
    "normalized = (importance_df[cols_to_use] - importance_df[cols_to_use].mean()) / importance_df[cols_to_use].std()\n",
    "\n",
    "# --- Define weights ---\n",
    "weights = {\n",
    "    \"shap_mean_abs\": 0.30,\n",
    "    \"perm_importance_mean\": 0.30,\n",
    "    \"lgbm_gain_importance\": 0.15,\n",
    "    \"mutual_information\": 0.10,\n",
    "    \"pearson_with_target\": 0.10,\n",
    "    \"lasso_coef_abs\": 0.05\n",
    "}\n",
    "\n",
    "# --- Composite score ---\n",
    "importance_df[\"composite_score\"] = sum(\n",
    "    normalized[col] * weights[col] for col in cols_to_use\n",
    ")\n",
    "\n",
    "# --- Rank features ---\n",
    "importance_df[\"rank\"] = importance_df[\"composite_score\"].rank(ascending=False)\n",
    "\n",
    "# --- Final ranked table ---\n",
    "ranked_features = importance_df.sort_values(\"composite_score\", ascending=False)\n",
    "\n",
    "ranked_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3d11c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_top_n_features(ranked_df, n):\n",
    "    return ranked_df.head(n).index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7510d68e",
   "metadata": {},
   "source": [
    "# Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f8dd50b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running forward CV for feature set: top15 (15 features) ===\n",
      "\n",
      "top15 - GLOBAL pooled OOS IC: 0.0269\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from xgboost import XGBRegressor\n",
    "import random\n",
    "\n",
    "\n",
    "def forward_cv_xgb(\n",
    "    X,\n",
    "    Y,\n",
    "    feature_cols,\n",
    "    half_life=100,\n",
    "    fractions=(0.5, 0.6, 0.7, 0.8, 0.9, 1.0),\n",
    "    xgb_params=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Rolling forward CV:\n",
    "    - Train on [0 : frac[i]]\n",
    "    - Test  on [frac[i] : frac[i+1]]\n",
    "    with exponential time-decay sample weights.\n",
    "\n",
    "    Returns:\n",
    "        fold_results_df: per-fold ICs, etc.\n",
    "        global_ic: pooled IC over all test folds concatenated\n",
    "    \"\"\"\n",
    "\n",
    "    X = X.reset_index(drop=True)\n",
    "    Y = Y.reset_index(drop=True)\n",
    "    n_samples = len(X)\n",
    "\n",
    "    if xgb_params is None:\n",
    "        xgb_params = dict(\n",
    "            objective=\"reg:squarederror\",\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=4,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            tree_method=\"hist\",\n",
    "            random_state=0,\n",
    "            eval_metric=\"rmse\",\n",
    "        )\n",
    "\n",
    "    # Turn fractions into actual indices\n",
    "    frac_arr = np.array(fractions)\n",
    "    assert np.allclose(frac_arr[-1], 1.0), \"Last fraction should be 1.0\"\n",
    "    boundaries = (frac_arr * n_samples).astype(int)\n",
    "\n",
    "    fold_results = []\n",
    "    all_y = []\n",
    "    all_pred = []\n",
    "\n",
    "    # Each fold: train [0:boundaries[i]], test [boundaries[i]:boundaries[i+1]]\n",
    "    for i in range(len(boundaries) - 1):\n",
    "        train_end = boundaries[i]\n",
    "        test_start = train_end\n",
    "        test_end = boundaries[i + 1]\n",
    "\n",
    "        train_idx = np.arange(0, train_end)\n",
    "        test_idx = np.arange(test_start, test_end)\n",
    "\n",
    "        X_train_fold = X.loc[train_idx, feature_cols]\n",
    "        Y_train_fold = Y.loc[train_idx]\n",
    "        X_test_fold = X.loc[test_idx, feature_cols]\n",
    "        Y_test_fold = Y.loc[test_idx]\n",
    "\n",
    "        # fold-local time decay\n",
    "        age_from_train_end = (train_end - 1) - train_idx\n",
    "        w_train_fold = 0.5 ** (age_from_train_end / half_life)\n",
    "\n",
    "        model = XGBRegressor(**xgb_params)\n",
    "\n",
    "        model.fit(\n",
    "            X_train_fold,\n",
    "            Y_train_fold,\n",
    "            sample_weight=w_train_fold,\n",
    "            eval_set=[(X_test_fold, Y_test_fold)],\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # Predict on the test fold\n",
    "        y_pred_test = model.predict(X_test_fold)\n",
    "        ic = pearsonr(y_pred_test, Y_test_fold)[0]\n",
    "\n",
    "        # store per-fold results\n",
    "        fold_results.append(\n",
    "            {\n",
    "                \"fold\": i,\n",
    "                \"n_train\": len(train_idx),\n",
    "                \"n_test\": len(test_idx),\n",
    "                \"test_ic\": ic,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # store for pooled IC\n",
    "        all_y.append(Y_test_fold.values)\n",
    "        all_pred.append(y_pred_test)\n",
    "\n",
    "    fold_results_df = pd.DataFrame(fold_results)\n",
    "\n",
    "    # pooled OOS IC across all test points\n",
    "    all_y = np.concatenate(all_y)\n",
    "    all_pred = np.concatenate(all_pred)\n",
    "    global_ic = pearsonr(all_pred, all_y)[0]\n",
    "\n",
    "    return fold_results_df, global_ic\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Run for N = 5, 10, 15, 20 features\n",
    "# -----------------------------\n",
    "top5 = sel_top_n_features(ranked_features, 5)\n",
    "top10 = sel_top_n_features(ranked_features, 10)\n",
    "top12 = sel_top_n_features(ranked_features, 12)\n",
    "top15 = sel_top_n_features(ranked_features, 15)\n",
    "top20 = sel_top_n_features(ranked_features, 20)\n",
    "\n",
    "feature_sets = {\n",
    "    # \"top5\": top5,\n",
    "    # \"top10\": top10,\n",
    "    # \"top12\": top12,\n",
    "    \"top15\": top15,\n",
    "    # \"top20\": top20,\n",
    "}\n",
    "\n",
    "all_summaries = {}\n",
    "\n",
    "for name, feats in feature_sets.items():\n",
    "    print(f\"\\n=== Running forward CV for feature set: {name} ({len(feats)} features) ===\")\n",
    "    folds_df, global_ic = forward_cv_xgb(\n",
    "        X=X,\n",
    "        Y=Y,\n",
    "        feature_cols=feats,\n",
    "        half_life=500,\n",
    "        fractions=(0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 1.0),  # 5 folds\n",
    "        xgb_params = dict(\n",
    "            objective=\"reg:squarederror\",\n",
    "            n_estimators=9500,\n",
    "            learning_rate=0.005,\n",
    "\n",
    "            # Tree complexity\n",
    "            max_depth=3,\n",
    "            max_leaves=32,             # optional\n",
    "            min_child_weight=2,       # VERY important\n",
    "            gamma=0.0,                   # VERY important\n",
    "\n",
    "            # Regularisation\n",
    "            reg_alpha=0.01,             # L1\n",
    "            reg_lambda=0.5,            # L2\n",
    "\n",
    "            # Sampling\n",
    "            subsample=0.1,\n",
    "            colsample_bytree=0.5,\n",
    "            colsample_bylevel=0.5,\n",
    "            colsample_bynode=0.5,\n",
    "\n",
    "            tree_method=\"hist\",\n",
    "\n",
    "            # For variability analysis\n",
    "            random_state=random.randint(0, 10000),\n",
    ")\n",
    "\n",
    "    )\n",
    "\n",
    "    all_summaries[name] = folds_df\n",
    "\n",
    "    # Mean IC across folds\n",
    "    mean_ic = folds_df[\"test_ic\"].mean()\n",
    "    # std_ic = folds_df[\"test_ic\"].std()\n",
    "\n",
    "    # Weights for weighted mean (more recent folds get higher weight) – 4 folds → 4 weights\n",
    "    #weights = np.array([0.15, 0.2, 0.25, 0.3])\n",
    "    #weighted_mean_ic = np.average(folds_df[\"test_ic\"], weights=weights)\n",
    "\n",
    "    # Last fold IC (most recent period)\n",
    "    # last_fold_ic = folds_df.iloc[-1][\"test_ic\"]\n",
    "\n",
    "    print(f\"\\n{name} - GLOBAL pooled OOS IC: {global_ic:.4f}\")\n",
    "    # print(f\"{name} - mean IC across folds: {mean_ic:.4f} ± {std_ic:.4f}\")\n",
    "    # print(f\"{name} - weighted mean IC across folds: {weighted_mean_ic:.4f}\")\n",
    "    # print(f\"{name} - last fold (most recent) IC: {last_fold_ic:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4773c439",
   "metadata": {},
   "source": [
    "# FINAL MODEL CHOSEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "524bb9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directional accuracy: 0.5367\n",
      "Baseline random (50%): 0.5000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Select final features\n",
    "# -----------------------------\n",
    "final_features = sel_top_n_features(ranked_features, 15)\n",
    "\n",
    "X_final = X[final_features].reset_index(drop=True)\n",
    "Y_final = Y.reset_index(drop=True)\n",
    "\n",
    "n_samples = len(X_final)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Exponential time-decay weights (global, anchored at end)\n",
    "# -----------------------------\n",
    "half_life = 500\n",
    "\n",
    "age_from_now = (n_samples - 1) - np.arange(n_samples)\n",
    "sample_weights = 0.5 ** (age_from_now / half_life)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Final XGBoost model\n",
    "# -----------------------------\n",
    "model = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_estimators=1500,\n",
    "    learning_rate=0.005,\n",
    "\n",
    "    # Tree complexity\n",
    "    max_depth=3,\n",
    "    max_leaves=32,\n",
    "    min_child_weight=2,\n",
    "    gamma=0.0,\n",
    "\n",
    "    # Regularisation\n",
    "    reg_alpha=0.01,\n",
    "    reg_lambda=0.5,\n",
    "\n",
    "    # Sampling\n",
    "    subsample=0.1,\n",
    "    colsample_bytree=0.5,\n",
    "    colsample_bylevel=0.5,\n",
    "    colsample_bynode=0.5,\n",
    "\n",
    "    tree_method=\"hist\",\n",
    "\n",
    "    random_state=0,   # fix seed for deployment\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Fit final model\n",
    "# -----------------------------\n",
    "model.fit(\n",
    "    X_final,\n",
    "    Y_final,\n",
    "    sample_weight=sample_weights,\n",
    ")\n",
    "\n",
    "preds = model.predict(X_final)\n",
    "\n",
    "y_true = Y_final.values\n",
    "mask = preds != 0\n",
    "\n",
    "directional_accuracy = np.mean(\n",
    "    np.sign(preds[mask]) == np.sign(y_true[mask])\n",
    ")\n",
    "\n",
    "print(f\"Directional accuracy: {directional_accuracy:.4f}\")\n",
    "print(f\"Baseline random (50%): 0.5000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d67affa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directional accuracy (preds < 0.005): 0.8889\n"
     ]
    }
   ],
   "source": [
    "mask = preds > 0.00\n",
    "directional_accuracy = np.mean(\n",
    "    np.sign(preds[mask]) == np.sign(y_true[mask])\n",
    ")\n",
    "print(f\"Directional accuracy (preds < 0.005): {directional_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9c3e35bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "7bfeffd6-78f7-49b5-98f6-268bd3b6e14f",
       "rows": [
        [
         "count",
         "8989.0"
        ],
        [
         "mean",
         "0.0001493787713116035"
        ],
        [
         "std",
         "0.0008106309687718749"
        ],
        [
         "min",
         "-0.004270122852176428"
        ],
        [
         "25%",
         "-0.0003843756567221135"
        ],
        [
         "50%",
         "0.00012072120443917811"
        ],
        [
         "75%",
         "0.0006681036320514977"
        ],
        [
         "max",
         "0.004598499741405249"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "count    8989.000000\n",
       "mean        0.000149\n",
       "std         0.000811\n",
       "min        -0.004270\n",
       "25%        -0.000384\n",
       "50%         0.000121\n",
       "75%         0.000668\n",
       "max         0.004598\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10th percentile: -0.0009\n",
      " 90th percentile: 0.0012\n",
      " 99th percentile: 0.0022\n"
     ]
    }
   ],
   "source": [
    "display(pd.Series(preds).describe())\n",
    "plt.hist(preds, bins=50)\n",
    "plt.title(\"Histogram of final model predictions\")\n",
    "plt.xlabel(\"Predicted value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "print(f\" 10th percentile: {np.percentile(preds, 10):.4f}\")\n",
    "print(f\" 90th percentile: {np.percentile(preds, 90):.4f}\")\n",
    "print(f\" 99th percentile: {np.percentile(preds, 99):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3753698c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backtest metrics:\n",
      "  adjusted_sharpe: 0.5766457243048353\n",
      "  sharpe: 0.5839793705346945\n",
      "  strategy_vol_pct: 14.514187325661087\n",
      "  market_vol_pct: 17.688609036349533\n",
      "  vol_penalty: 1.0\n",
      "  return_penalty: 1.012717767462336\n",
      "  strategy_mean_excess_daily_geo: 0.00033634864993103974\n",
      "  market_mean_excess_daily_geo: 0.0003810998719591119\n",
      "  oos_ic_pred_vs_target: 0.020190794481045073\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "D1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "D2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "D3",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "D4",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "D5",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "D6",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "D7",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "D8",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "D9",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "E1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E11",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E12",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E13",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E14",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E16",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E17",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E18",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E19",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E20",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "E9",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "I1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "I2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "I3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "I4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "I5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "I6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "I7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "I8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "I9",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "M1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "M10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "M11",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "M12",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "M13",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "M14",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "M15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "M16",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "M17",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "M18",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "M2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "M3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "M4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "M5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "M6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "M7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "M8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "M9",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "P1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "P10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "P11",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "P12",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "P13",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "P2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "P3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "P4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "P5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "P6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "P7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "P8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "P9",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S11",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S12",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S9",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V11",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V12",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V13",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V7",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V8",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "V9",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "forward_returns",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "risk_free_rate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "market_forward_excess_returns",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "missing_indicator",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "target",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "prediction",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "position",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "strategy_returns",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "strategy_excess_returns",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "market_excess_returns",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "8ef918c6-38be-49cc-a3a5-2585a08c8b1c",
       "rows": [
        [
         "4494",
         "4494",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "0",
         "1",
         "0",
         "1.07581255697265",
         "0.915674603174603",
         "0.100198412698413",
         "0.186838624338624",
         "0.0201719576719577",
         "0.0201719576719577",
         "0.128637566137566",
         "-0.519629042366063",
         "-0.511130709296061",
         "-0.0898730679230045",
         "-0.269390644968155",
         "-0.427018839059801",
         "1.28159304879142",
         "-0.7367633676870619",
         "0.0717592592592593",
         "0.62202380952381",
         "0.021494708994708997",
         "-0.075476232687905",
         "-0.440987963005963",
         "0.277116402116402",
         "0.991732804232804",
         "-0.500155536692126",
         "0.017857142857142905",
         "0.55787037037037",
         "0.795208678056155",
         "0.10912698412698402",
         "0.57473544973545",
         "0.372354497354497",
         "0.799704673259864",
         "-0.6323342058138726",
         "-0.707373422819352",
         "0.8878320205596651",
         "2.2756104302663402",
         "-0.9283152127459174",
         "-1.0167329990980174",
         "0.234457671957672",
         "0.0006613756613756128",
         "0.0826719576719577",
         "0.305224867724868",
         "-0.40340564738650897",
         "-0.38341744709968695",
         "0.802737774813881",
         "-1.98059044257551",
         "0.5978925852033085",
         "0.0750940312958023",
         "0.163690476190476",
         "-0.5807482676265501",
         "0.16269841269841306",
         "1.99715845805301",
         "1.97173970117499",
         "1.7084340675825",
         "0.14484126984127",
         "0.525710769371931",
         "0.779100529100529",
         "0.290674603174603",
         "0.617321648065053",
         "0.0611840238570618",
         "0.554186762221938",
         "1.75429302126257",
         "0.917989417989418",
         "4.00453099541517",
         "0.17989417989418",
         "0.187169312169312",
         "0.501512388996307",
         "0.106672821361748",
         "0.400653192758131",
         "0.599206349206349",
         "-0.43657101628196104",
         "0.303571428571429",
         "0.701388888888889",
         "0.627722584563815",
         "0.408730158730159",
         "0.466931216931217",
         "-0.5332053502319859",
         "0.0006613756613756128",
         "0.13822751322751303",
         "0.0745356885706614",
         "0.837962962962963",
         "0.857142857142857",
         "0.776455026455027",
         "-0.33257025829225",
         "0.466931216931217",
         "0.262908846991988",
         "0.138227513227513",
         "0.6275259794834935",
         "-0.0069421918610013",
         "0.0001531746031746",
         "-0.0074067945705069",
         "1",
         "0.0099237901931828",
         "-0.0008896684157662094",
         "0.0",
         "0.0001531746031746",
         "0.0",
         "-0.0070953664641759005"
        ],
        [
         "4495",
         "4495",
         "1",
         "1",
         "0",
         "1",
         "1",
         "0",
         "0",
         "1",
         "0",
         "1.07439945899118",
         "0.9160052910052909",
         "0.0998677248677249",
         "0.186507936507937",
         "0.0198412698412698",
         "0.0198412698412698",
         "0.128968253968254",
         "-0.519643798760805",
         "-0.512247366866369",
         "-0.09152383946981908",
         "-0.560501918633847",
         "-0.521550779893268",
         "1.27996864900065",
         "-0.81983099384664",
         "0.0714285714285714",
         "0.622354497354497",
         "0.021164021164021204",
         "-0.13262232116118935",
         "-0.439962038012573",
         "0.276785714285714",
         "0.990079365079365",
         "-0.530721159552876",
         "0.012566137566137559",
         "0.551917989417989",
         "0.787752966960942",
         "0.10846560846560799",
         "0.53968253968254",
         "0.354497354497354",
         "0.793833543483369",
         "-1.3164377049706149",
         "-0.706508760308542",
         "0.9096794541735519",
         "2.3531845740105",
         "-1.65438972700627",
         "-0.929437377238243",
         "0.23412698412698402",
         "0.0006613756613756128",
         "0.06812169312169308",
         "0.305555555555556",
         "0.8819037526968881",
         "0.218180564137507",
         "0.356046343242096",
         "-1.99553124883201",
         "0.473831324232611",
         "0.0730618211659804",
         "0.02149470899470901",
         "-0.591396961677142",
         "0.172619047619048",
         "1.96617745826931",
         "1.91835106754298",
         "0.204494881786257",
         "0.23445767195767198",
         "0.536281148927759",
         "0.060846560846560815",
         "0.979497354497355",
         "0.948217079987492",
         "-0.18430523132139104",
         "-0.8171929298154799",
         "1.7385644983202",
         "0.904100529100529",
         "3.9063751224608",
         "0.46957671957672",
         "0.516534391534392",
         "0.470515952784101",
         "-0.0248946071924522",
         "-0.107407326113229",
         "0.10449735449735398",
         "-0.38823204787727694",
         "0.303902116402116",
         "0.701719576719577",
         "0.673277550488861",
         "0.18452380952380998",
         "0.467592592592593",
         "-1.0363264344481116",
         "0.0006613756613756128",
         "0.138888888888889",
         "0.3880998848316519",
         "0.917989417989418",
         "0.865740740740741",
         "0.814814814814815",
         "-0.5512917580474739",
         "0.467592592592593",
         "0.43860145164333797",
         "0.138888888888889",
         "0.763644145822235",
         "0.010388083378879",
         "0.0001527777777777",
         "0.0099237901931828",
         "1",
         "-0.0238703253882673",
         "0.000587965885642916",
         "1.0",
         "0.010388083378879",
         "0.0102353056011013",
         "0.0102353056011013"
        ],
        [
         "4496",
         "4496",
         "0",
         "0",
         "0",
         "1",
         "1",
         "0",
         "0",
         "1",
         "0",
         "1.07298853394195",
         "0.916335978835979",
         "0.099537037037037",
         "0.186177248677249",
         "0.019510582010582",
         "0.019510582010581996",
         "0.12929894179894202",
         "-0.519658555425334",
         "-0.513366312910697",
         "-0.09318110757384619",
         "0.721121410452231",
         "-0.357561195956804",
         "1.28456331255697",
         "-0.68023062509639",
         "0.0710978835978836",
         "0.622685185185185",
         "0.0208333333333333",
         "-0.19993796259555524",
         "-0.43893579495573",
         "0.276455026455026",
         "0.990079365079365",
         "-0.498074931766461",
         "0.009920634920634885",
         "0.558201058201058",
         "0.767177645856035",
         "0.11574074074074098",
         "0.485449735449735",
         "0.313492063492063",
         "0.766232365096171",
         "0.18631299217397257",
         "-0.70564469781636",
         "0.9853695626066009",
         "2.23032680982414",
         "0.07167123741295722",
         "-1.107851025863833",
         "0.272156084656085",
         "0.0006613756613756128",
         "0.06812169312169308",
         "0.305886243386243",
         "-1.4233471971816298",
         "1.67812510871732",
         "1.19868667883603",
         "-2.08791523521469",
         "-0.26415082556702246",
         "0.0710151296090193",
         "0.398148148148148",
         "-0.571836374729299",
         "0.14285714285714302",
         "2.00518140236647",
         "2.008887498958",
         "0.956072839971634",
         "0.10482804232804194",
         "0.397476769339655",
         "0.06051587301587302",
         "0.443121693121693",
         "-0.326765020819048",
         "0.0612218876374582",
         "1.61014821036118",
         "1.77111530513607",
         "0.8835978835978842",
         "3.76732966120355",
         "0.693783068783069",
         "0.696428571428571",
         "0.951706193189761",
         "-0.09032340072600911",
         "-0.09212726319119605",
         "0.238756613756614",
         "-2.7015768171085606",
         "0.55489417989418",
         "0.702050264550265",
         "0.65755154372943",
         "0.457671957671958",
         "0.468253968253968",
         "0.35591596083451515",
         "0.0006613756613756128",
         "0.139550264550265",
         "-0.18444138674139396",
         "0.911375661375661",
         "0.908068783068783",
         "0.831349206349206",
         "-0.689693907064099",
         "0.468253968253968",
         "0.0699550344772151",
         "0.139550264550265",
         "0.677580186269386",
         "-0.0234076949009838",
         "0.0001515873015873",
         "-0.0238703253882673",
         "1",
         "0.0006675131884877",
         "-0.00030624610371887684",
         "0.0",
         "0.0001515873015873",
         "0.0",
         "-0.0235592822025711"
        ],
        [
         "4497",
         "4497",
         "0",
         "0",
         "0",
         "1",
         "1",
         "0",
         "1",
         "0",
         "1",
         "1.07157977340088",
         "0.916666666666667",
         "0.0992063492063492",
         "0.185846560846561",
         "0.0191798941798942",
         "0.019179894179894196",
         "0.1296296296296301",
         "-0.519673312359666",
         "-0.514487557790573",
         "-0.0948449380248864",
         "1.32869191369234",
         "-0.735236114615378",
         "1.28294080419471",
         "-1.00675906037293",
         "0.07076719576719581",
         "0.623015873015873",
         "0.02050264550264551",
         "-0.1850428048725346",
         "-0.57256302942402",
         "0.276124338624339",
         "0.989087301587302",
         "-0.370423384893677",
         "0.000661375661375585",
         "0.55787037037037",
         "0.690862756480789",
         "0.11640211640211601",
         "0.345238095238095",
         "0.307539682539683",
         "0.692451488039003",
         "1.18584383075151",
         "-0.704781233381162",
         "0.82011447710169",
         "2.38470362230396",
         "0.19610446111318847",
         "-0.753169867499553",
         "0.291335978835979",
         "0.0006613756613756128",
         "0.0671296296296296",
         "0.306216931216931",
         "2.47865487723843",
         "0.104727275799573",
         "-0.288114881825884",
         "-1.9331249554152399",
         "-0.01258529806551184",
         "0.0689536728332936",
         "0.846230158730159",
         "-0.618733306510523",
         "0.10416666666666702",
         "1.89366446306458",
         "1.80193337728232",
         "1.20512428061525",
         "0.02149470899470901",
         "0.9327370505717649",
         "0.355489417989418",
         "0.613756613756614",
         "0.668792464217559",
         "0.0323887182435225",
         "-2.3907915076072803",
         "1.69432484598276",
         "0.884920634920635",
         "3.71013713722702",
         "0.23015873015873",
         "0.125",
         "1.55755213451885",
         "0.174088414368386",
         "-1.4664266557814751",
         "0.06481481481481483",
         "-1.673590408424",
         "0.408068783068783",
         "0.702380952380952",
         "0.809778708951272",
         "0.673280423280423",
         "0.468915343915344",
         "1.5926670064664452",
         "0.0006613756613756128",
         "0.14021164021164",
         "0.770653645045411",
         "0.898809523809524",
         "0.90542328042328",
         "0.817460317460317",
         "-0.22821586619469997",
         "0.468915343915344",
         "0.754681678901573",
         "0.14021164021164",
         "0.8640702248155063",
         "0.0011256039932314",
         "0.0001470238095238",
         "0.0006675131884877",
         "1",
         "-0.0080560614987448",
         "-0.00011516256199683994",
         "0.0",
         "0.0001470238095238",
         "0.0",
         "0.0009785801837076"
        ],
        [
         "4498",
         "4498",
         "0",
         "0",
         "0",
         "1",
         "1",
         "0",
         "0",
         "0",
         "1",
         "1.07017316898322",
         "0.916997354497355",
         "0.0988756613756614",
         "0.185515873015873",
         "0.0188492063492063",
         "0.0188492063492063",
         "0.129960317460317",
         "-0.519688069563815",
         "-0.515611111932549",
         "-0.09651539754335871",
         "1.9919314690824603",
         "-0.7277249328090502",
         "1.28276847382603",
         "-1.0015150896790401",
         "0.0704365079365079",
         "0.623346560846561",
         "0.020171957671957702",
         "-0.1636707216314315",
         "-0.571773993673858",
         "0.275793650793651",
         "0.973875661375661",
         "-0.527816010406218",
         "0.003968253968253954",
         "0.562169312169312",
         "0.55561030767256",
         "0.11838624338624304",
         "0.351851851851852",
         "0.326719576719577",
         "0.565615322893009",
         "-0.762719841987654",
         "-0.771931761012071",
         "1.03843492640032",
         "2.33546371665706",
         "-1.4282419107299549",
         "-0.7461248717166336",
         "0.322751322751323",
         "0.0006613756613756128",
         "0.0667989417989418",
         "0.306547619047619",
         "0.258112208378697",
         "-0.8562991555672439",
         "-0.305577984532528",
         "-2.10312313131668",
         "1.4960365580920332",
         "0.22042965009849",
         "0.36276455026455",
         "-0.6169820425676631",
         "0.08994708994708989",
         "1.90544321147511",
         "1.80440776546472",
         "5.175871690494",
         "0.103835978835979",
         "0.867170982280784",
         "0.880621693121693",
         "0.365740740740741",
         "-0.705305428993623",
         "1.37509473771122",
         "-0.209061269320975",
         "1.7100984872147",
         "0.906084656084656",
         "3.6753240577589406",
         "0.021825396825396803",
         "0.0033068783068783136",
         "-0.596410527262777",
         "-0.419809842473628",
         "0.011880332384991553",
         "0.380952380952381",
         "-0.867300613275668",
         "0.529100529100529",
         "0.70271164021164",
         "0.748904962689723",
         "0.539021164021164",
         "0.46957671957672",
         "-0.8875860514971424",
         "0.0006613756613756128",
         "0.140873015873016",
         "0.561487331877039",
         "0.903439153439153",
         "0.914021164021164",
         "0.845899470899471",
         "-0.649079608722591",
         "0.46957671957672",
         "0.727597267768531",
         "0.140873015873016",
         "0.9342127475832256",
         "-0.0076058199003502",
         "0.0001388888888888",
         "-0.0080560614987448",
         "1",
         "0.0130070241653685",
         "0.00023397689801640809",
         "1.0",
         "-0.0076058199003502",
         "-0.007744708789239",
         "-0.007744708789239"
        ]
       ],
       "shape": {
        "columns": 105,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>...</th>\n",
       "      <th>forward_returns</th>\n",
       "      <th>risk_free_rate</th>\n",
       "      <th>market_forward_excess_returns</th>\n",
       "      <th>missing_indicator</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "      <th>position</th>\n",
       "      <th>strategy_returns</th>\n",
       "      <th>strategy_excess_returns</th>\n",
       "      <th>market_excess_returns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4494</th>\n",
       "      <td>4494</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006942</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>-0.007407</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009924</td>\n",
       "      <td>-0.000890</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.007095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4495</th>\n",
       "      <td>4495</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010388</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.009924</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.023870</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010388</td>\n",
       "      <td>0.010235</td>\n",
       "      <td>0.010235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>4496</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023408</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>-0.023870</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>-0.000306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.023559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>4497</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.008056</td>\n",
       "      <td>-0.000115</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4498</th>\n",
       "      <td>4498</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007606</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>-0.008056</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013007</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.007606</td>\n",
       "      <td>-0.007745</td>\n",
       "      <td>-0.007745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date_id  D1  D2  D3  D4  D5  D6  D7  D8  D9  ...  forward_returns  \\\n",
       "4494     4494   0   0   0   1   0   0   0   1   0  ...        -0.006942   \n",
       "4495     4495   1   1   0   1   1   0   0   1   0  ...         0.010388   \n",
       "4496     4496   0   0   0   1   1   0   0   1   0  ...        -0.023408   \n",
       "4497     4497   0   0   0   1   1   0   1   0   1  ...         0.001126   \n",
       "4498     4498   0   0   0   1   1   0   0   0   1  ...        -0.007606   \n",
       "\n",
       "      risk_free_rate  market_forward_excess_returns  missing_indicator  \\\n",
       "4494        0.000153                      -0.007407                  1   \n",
       "4495        0.000153                       0.009924                  1   \n",
       "4496        0.000152                      -0.023870                  1   \n",
       "4497        0.000147                       0.000668                  1   \n",
       "4498        0.000139                      -0.008056                  1   \n",
       "\n",
       "        target  prediction  position  strategy_returns  \\\n",
       "4494  0.009924   -0.000890       0.0          0.000153   \n",
       "4495 -0.023870    0.000588       1.0          0.010388   \n",
       "4496  0.000668   -0.000306       0.0          0.000152   \n",
       "4497 -0.008056   -0.000115       0.0          0.000147   \n",
       "4498  0.013007    0.000234       1.0         -0.007606   \n",
       "\n",
       "      strategy_excess_returns  market_excess_returns  \n",
       "4494                 0.000000              -0.007095  \n",
       "4495                 0.010235               0.010235  \n",
       "4496                 0.000000              -0.023559  \n",
       "4497                 0.000000               0.000979  \n",
       "4498                -0.007745              -0.007745  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "MIN_INVESTMENT = 0.0\n",
    "MAX_INVESTMENT = 2.0\n",
    "TRADING_DAYS_PER_YR = 252\n",
    "\n",
    "# -----------------------------\n",
    "# Baseline allocation: linear ramp with cap\n",
    "# w_t = clip(yhat / scale, 0, 2), with scale calibrated from train preds\n",
    "# -----------------------------\n",
    "def alloc_on_off(yhat: np.ndarray, w_on: float = 1.0) -> np.ndarray:\n",
    "    return np.where(yhat > 0, w_on, 0.0)\n",
    "\n",
    "# -----------------------------\n",
    "# Metric (matches your score() logic)\n",
    "# -----------------------------\n",
    "def adjusted_sharpe_metric(df: pd.DataFrame) -> dict:\n",
    "    # df must have columns: position, forward_returns, risk_free_rate\n",
    "    w = df[\"position\"].values\n",
    "    rf = df[\"risk_free_rate\"].values\n",
    "    r = df[\"forward_returns\"].values\n",
    "\n",
    "    strategy_returns = rf * (1 - w) + w * r\n",
    "    strategy_excess = strategy_returns - rf\n",
    "\n",
    "    # geometric mean daily excess return\n",
    "    strat_excess_cum = np.prod(1.0 + strategy_excess)\n",
    "    strat_mean_excess = strat_excess_cum ** (1.0 / len(df)) - 1.0\n",
    "\n",
    "    strat_std = np.std(strategy_returns, ddof=1)\n",
    "    if strat_std == 0:\n",
    "        raise ValueError(\"Strategy std is zero -> Sharpe undefined\")\n",
    "\n",
    "    sharpe = (strat_mean_excess / strat_std) * np.sqrt(TRADING_DAYS_PER_YR)\n",
    "    strat_vol = float(strat_std * np.sqrt(TRADING_DAYS_PER_YR) * 100.0)\n",
    "\n",
    "    # market stats\n",
    "    market_excess = r - rf\n",
    "    market_excess_cum = np.prod(1.0 + market_excess)\n",
    "    market_mean_excess = market_excess_cum ** (1.0 / len(df)) - 1.0\n",
    "\n",
    "    market_std = np.std(r, ddof=1)\n",
    "    if market_std == 0:\n",
    "        raise ValueError(\"Market std is zero -> penalty undefined\")\n",
    "\n",
    "    market_vol = float(market_std * np.sqrt(TRADING_DAYS_PER_YR) * 100.0)\n",
    "\n",
    "    # penalties\n",
    "    excess_vol = max(0.0, strat_vol / market_vol - 1.2)\n",
    "    vol_penalty = 1.0 + excess_vol\n",
    "\n",
    "    return_gap = max(0.0, (market_mean_excess - strat_mean_excess) * 100.0 * TRADING_DAYS_PER_YR)\n",
    "    return_penalty = 1.0 + (return_gap**2) / 100.0\n",
    "\n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "\n",
    "    return {\n",
    "        \"adjusted_sharpe\": float(min(adjusted_sharpe, 1_000_000)),\n",
    "        \"sharpe\": float(sharpe),\n",
    "        \"strategy_vol_pct\": float(strat_vol),\n",
    "        \"market_vol_pct\": float(market_vol),\n",
    "        \"vol_penalty\": float(vol_penalty),\n",
    "        \"return_penalty\": float(return_penalty),\n",
    "        \"strategy_mean_excess_daily_geo\": float(strat_mean_excess),\n",
    "        \"market_mean_excess_daily_geo\": float(market_mean_excess),\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Rolling refit backtest\n",
    "# - train on first half\n",
    "# - then walk forward, refit every refit_every rows\n",
    "# - predict next chunk, allocate positions, compute backtest stats\n",
    "# -----------------------------\n",
    "def rolling_backtest_baseline(\n",
    "    full_train: pd.DataFrame,\n",
    "    feature_cols: list[str],\n",
    "    xgb_params: dict,\n",
    "    half_life: float = 500.0,\n",
    "    refit_every: int = 100,\n",
    "    scale_quantile: float = 0.90,\n",
    "):\n",
    "    df = full_train.reset_index(drop=True).copy()\n",
    "    n = len(df)\n",
    "\n",
    "    # split: first half train only, second half is OOS simulation horizon\n",
    "    split = n // 2\n",
    "    oos_start = split\n",
    "\n",
    "    # containers for OOS predictions + positions\n",
    "    oos_pred = np.full(n, np.nan, dtype=float)\n",
    "    oos_pos = np.full(n, np.nan, dtype=float)\n",
    "\n",
    "    # walk-forward\n",
    "    t = oos_start\n",
    "    while t < n:\n",
    "        # expanding train set up to t-1\n",
    "        train_end = t  # exclusive\n",
    "        X_train = df.loc[:train_end - 1, feature_cols]\n",
    "        y_train = df.loc[:train_end - 1, \"target\"]\n",
    "\n",
    "        # time-decay weights anchored at train_end-1 (local to this refit)\n",
    "        idx = np.arange(train_end)\n",
    "        age = (train_end - 1) - idx\n",
    "        w_train = 0.5 ** (age / half_life)\n",
    "\n",
    "        model = XGBRegressor(**xgb_params)\n",
    "        model.fit(X_train, y_train, sample_weight=w_train, verbose=False)\n",
    "\n",
    "        # predict next chunk\n",
    "        chunk_end = min(t + refit_every, n)\n",
    "        X_chunk = df.loc[t:chunk_end - 1, feature_cols]\n",
    "        yhat_chunk = model.predict(X_chunk)\n",
    "\n",
    "        # calibrate allocation scale from (in-sample) training predictions\n",
    "        yhat_train = model.predict(X_train)\n",
    "        pos_train = yhat_train[yhat_train > 0]\n",
    "        scale = np.quantile(pos_train, scale_quantile) if len(pos_train) else 1.0\n",
    "        if scale <= 0 or not np.isfinite(scale):\n",
    "            scale = 1.0\n",
    "\n",
    "        pos_chunk = alloc_on_off(yhat_chunk, w_on=1.0)\n",
    "\n",
    "        # store\n",
    "        oos_pred[t:chunk_end] = yhat_chunk\n",
    "        oos_pos[t:chunk_end] = pos_chunk\n",
    "\n",
    "        t = chunk_end\n",
    "\n",
    "    # build OOS results frame (only second half)\n",
    "    oos = df.loc[oos_start:].copy()\n",
    "    oos[\"prediction\"] = oos_pred[oos_start:]\n",
    "    oos[\"position\"] = oos_pos[oos_start:]\n",
    "\n",
    "    # sanity: enforce bounds\n",
    "    oos[\"position\"] = oos[\"position\"].clip(MIN_INVESTMENT, MAX_INVESTMENT)\n",
    "\n",
    "    # compute backtest returns (same as score())\n",
    "    oos[\"strategy_returns\"] = oos[\"risk_free_rate\"] * (1 - oos[\"position\"]) + oos[\"position\"] * oos[\"forward_returns\"]\n",
    "    oos[\"strategy_excess_returns\"] = oos[\"strategy_returns\"] - oos[\"risk_free_rate\"]\n",
    "    oos[\"market_excess_returns\"] = oos[\"forward_returns\"] - oos[\"risk_free_rate\"]\n",
    "\n",
    "    # IC diagnostics (prediction vs target on OOS)\n",
    "    ic_pred_target = pearsonr(oos[\"prediction\"].values, oos[\"target\"].values)[0]\n",
    "\n",
    "    # adjusted Sharpe metric + components\n",
    "    metrics = adjusted_sharpe_metric(oos[[\"position\", \"forward_returns\", \"risk_free_rate\"]].copy())\n",
    "    metrics[\"oos_ic_pred_vs_target\"] = float(ic_pred_target)\n",
    "\n",
    "    return oos, metrics\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "final_features = sel_top_n_features(ranked_features, 15)\n",
    "\n",
    "# IMPORTANT: ensure these exist on full_train\n",
    "# - full_train[\"target\"] already defined as next-day market_forward_excess_returns\n",
    "# - full_train[\"forward_returns\"], full_train[\"risk_free_rate\"] exist\n",
    "# - full_train is time-ordered\n",
    "\n",
    "xgb_params = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_estimators=1500,\n",
    "    learning_rate=0.005,\n",
    "    max_depth=3,\n",
    "    max_leaves=32,\n",
    "    min_child_weight=2,\n",
    "    gamma=0.0,\n",
    "    reg_alpha=0.01,\n",
    "    reg_lambda=0.5,\n",
    "    subsample=0.1,\n",
    "    colsample_bytree=0.5,\n",
    "    colsample_bylevel=0.5,\n",
    "    colsample_bynode=0.5,\n",
    "    tree_method=\"hist\",\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "oos_df, bt_metrics = rolling_backtest_baseline(\n",
    "    full_train=full_train,\n",
    "    feature_cols=final_features,\n",
    "    xgb_params=xgb_params,\n",
    "    half_life=500,\n",
    "    refit_every=100,\n",
    "    scale_quantile=0.90,   # tweak: 0.8/0.9/0.95\n",
    ")\n",
    "\n",
    "print(\"Backtest metrics:\")\n",
    "for k, v in bt_metrics.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# oos_df now contains daily position + strategy_returns etc. for the OOS half\n",
    "oos_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "df6e1e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 (thresholded on/off) Backtest metrics:\n",
      "  adjusted_sharpe: 0.5860753919646481\n",
      "  sharpe: 0.5941595315848734\n",
      "  strategy_vol_pct: 14.186848998799197\n",
      "  market_vol_pct: 17.688609036349533\n",
      "  vol_penalty: 1.0\n",
      "  return_penalty: 1.0137936854729996\n",
      "  strategy_mean_excess_daily_geo: 0.0003344941093568199\n",
      "  market_mean_excess_daily_geo: 0.0003810998719591119\n",
      "  oos_ic_pred_vs_target: 0.020190794481045073\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "MIN_INVESTMENT = 0.0\n",
    "MAX_INVESTMENT = 2.0\n",
    "TRADING_DAYS_PER_YR = 252\n",
    "\n",
    "# -----------------------------\n",
    "# Layer 1 allocation: thresholded on/off\n",
    "# w_t = 1 if yhat > tau else 0\n",
    "# -----------------------------\n",
    "def alloc_threshold_on_off(yhat: np.ndarray, tau: float, w_on: float = 1.0) -> np.ndarray:\n",
    "    return np.where(yhat > tau, w_on, 0.0)\n",
    "\n",
    "# -----------------------------\n",
    "# Metric (matches your score() logic)\n",
    "# -----------------------------\n",
    "def adjusted_sharpe_metric(df: pd.DataFrame) -> dict:\n",
    "    w = df[\"position\"].values\n",
    "    rf = df[\"risk_free_rate\"].values\n",
    "    r = df[\"forward_returns\"].values\n",
    "\n",
    "    strategy_returns = rf * (1 - w) + w * r\n",
    "    strategy_excess = strategy_returns - rf\n",
    "\n",
    "    strat_excess_cum = np.prod(1.0 + strategy_excess)\n",
    "    strat_mean_excess = strat_excess_cum ** (1.0 / len(df)) - 1.0\n",
    "\n",
    "    strat_std = np.std(strategy_returns, ddof=1)\n",
    "    if strat_std == 0:\n",
    "        raise ValueError(\"Strategy std is zero -> Sharpe undefined\")\n",
    "\n",
    "    sharpe = (strat_mean_excess / strat_std) * np.sqrt(TRADING_DAYS_PER_YR)\n",
    "    strat_vol = float(strat_std * np.sqrt(TRADING_DAYS_PER_YR) * 100.0)\n",
    "\n",
    "    market_excess = r - rf\n",
    "    market_excess_cum = np.prod(1.0 + market_excess)\n",
    "    market_mean_excess = market_excess_cum ** (1.0 / len(df)) - 1.0\n",
    "\n",
    "    market_std = np.std(r, ddof=1)\n",
    "    if market_std == 0:\n",
    "        raise ValueError(\"Market std is zero -> penalty undefined\")\n",
    "\n",
    "    market_vol = float(market_std * np.sqrt(TRADING_DAYS_PER_YR) * 100.0)\n",
    "\n",
    "    excess_vol = max(0.0, strat_vol / market_vol - 1.2)\n",
    "    vol_penalty = 1.0 + excess_vol\n",
    "\n",
    "    return_gap = max(0.0, (market_mean_excess - strat_mean_excess) * 100.0 * TRADING_DAYS_PER_YR)\n",
    "    return_penalty = 1.0 + (return_gap**2) / 100.0\n",
    "\n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "\n",
    "    return {\n",
    "        \"adjusted_sharpe\": float(min(adjusted_sharpe, 1_000_000)),\n",
    "        \"sharpe\": float(sharpe),\n",
    "        \"strategy_vol_pct\": float(strat_vol),\n",
    "        \"market_vol_pct\": float(market_vol),\n",
    "        \"vol_penalty\": float(vol_penalty),\n",
    "        \"return_penalty\": float(return_penalty),\n",
    "        \"strategy_mean_excess_daily_geo\": float(strat_mean_excess),\n",
    "        \"market_mean_excess_daily_geo\": float(market_mean_excess),\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Rolling refit backtest (train on first half, refit every 100 rows)\n",
    "# -----------------------------\n",
    "def rolling_backtest_threshold_on_off(\n",
    "    full_train: pd.DataFrame,\n",
    "    feature_cols: list[str],\n",
    "    xgb_params: dict,\n",
    "    tau: float,                    # <-- TUNE THIS\n",
    "    half_life: float = 500.0,\n",
    "    refit_every: int = 100,\n",
    "    w_on: float = 1.0,\n",
    "):\n",
    "    df = full_train.reset_index(drop=True).copy()\n",
    "    n = len(df)\n",
    "\n",
    "    split = n // 2\n",
    "    oos_start = split\n",
    "\n",
    "    oos_pred = np.full(n, np.nan, dtype=float)\n",
    "    oos_pos  = np.full(n, np.nan, dtype=float)\n",
    "\n",
    "    t = oos_start\n",
    "    while t < n:\n",
    "        train_end = t\n",
    "\n",
    "        X_train = df.loc[:train_end - 1, feature_cols]\n",
    "        y_train = df.loc[:train_end - 1, \"target\"]\n",
    "\n",
    "        # fold-local time decay (anchored at train_end-1)\n",
    "        idx = np.arange(train_end)\n",
    "        age = (train_end - 1) - idx\n",
    "        w_train = 0.5 ** (age / half_life)\n",
    "\n",
    "        model = XGBRegressor(**xgb_params)\n",
    "        model.fit(X_train, y_train, sample_weight=w_train, verbose=False)\n",
    "\n",
    "        chunk_end = min(t + refit_every, n)\n",
    "        X_chunk = df.loc[t:chunk_end - 1, feature_cols]\n",
    "        yhat_chunk = model.predict(X_chunk)\n",
    "\n",
    "        pos_chunk = alloc_threshold_on_off(yhat_chunk, tau=tau, w_on=w_on)\n",
    "\n",
    "        oos_pred[t:chunk_end] = yhat_chunk\n",
    "        oos_pos[t:chunk_end]  = pos_chunk\n",
    "\n",
    "        t = chunk_end\n",
    "\n",
    "    oos = df.loc[oos_start:].copy()\n",
    "    oos[\"prediction\"] = oos_pred[oos_start:]\n",
    "    oos[\"position\"]   = np.clip(oos_pos[oos_start:], MIN_INVESTMENT, MAX_INVESTMENT)\n",
    "\n",
    "    oos[\"strategy_returns\"] = (\n",
    "        oos[\"risk_free_rate\"] * (1 - oos[\"position\"]) +\n",
    "        oos[\"position\"] * oos[\"forward_returns\"]\n",
    "    )\n",
    "\n",
    "    oos[\"strategy_excess_returns\"] = oos[\"strategy_returns\"] - oos[\"risk_free_rate\"]\n",
    "    oos[\"market_excess_returns\"] = oos[\"forward_returns\"] - oos[\"risk_free_rate\"]\n",
    "\n",
    "    ic_pred_target = pearsonr(oos[\"prediction\"].values, oos[\"target\"].values)[0]\n",
    "\n",
    "    metrics = adjusted_sharpe_metric(oos[[\"position\", \"forward_returns\", \"risk_free_rate\"]].copy())\n",
    "    metrics[\"oos_ic_pred_vs_target\"] = float(ic_pred_target)\n",
    "\n",
    "    return oos, metrics\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "final_features = sel_top_n_features(ranked_features, 15)\n",
    "\n",
    "xgb_params = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_estimators=1500,\n",
    "    learning_rate=0.005,\n",
    "    max_depth=3,\n",
    "    max_leaves=32,\n",
    "    min_child_weight=2,\n",
    "    gamma=0.0,\n",
    "    reg_alpha=0.01,\n",
    "    reg_lambda=0.5,\n",
    "    subsample=0.1,\n",
    "    colsample_bytree=0.5,\n",
    "    colsample_bylevel=0.5,\n",
    "    colsample_bynode=0.5,\n",
    "    tree_method=\"hist\",\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "tau = 0.0001  # <-- YOU TUNE THIS (try e.g. 0, 1e-5, 5e-5, 1e-4, 2e-4, ...)\n",
    "\n",
    "oos_df, bt_metrics = rolling_backtest_threshold_on_off(\n",
    "    full_train=full_train,\n",
    "    feature_cols=final_features,\n",
    "    xgb_params=xgb_params,\n",
    "    tau=tau,\n",
    "    half_life=500,\n",
    "    refit_every=100,\n",
    "    w_on=1.0,\n",
    ")\n",
    "\n",
    "print(\"Layer 1 (thresholded on/off) Backtest metrics:\")\n",
    "for k, v in bt_metrics.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "03ac048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double piecewise linear allocation metrics:\n",
      "  adjusted_sharpe: 0.548289251875198\n",
      "  sharpe: 0.5575649160228275\n",
      "  strategy_vol_pct: 14.891620658187293\n",
      "  market_vol_pct: 17.688609036349533\n",
      "  vol_penalty: 1.0\n",
      "  return_penalty: 1.01691746485255\n",
      "  strategy_mean_excess_daily_geo: 0.00032948592149706357\n",
      "  market_mean_excess_daily_geo: 0.0003810998719591119\n",
      "  oos_ic_pred_vs_target: 0.020190794481045073\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "MIN_INVESTMENT = 0.0\n",
    "MAX_INVESTMENT = 2.0\n",
    "TRADING_DAYS_PER_YR = 252\n",
    "\n",
    "# -----------------------------\n",
    "# Double piecewise linear allocation\n",
    "# -----------------------------\n",
    "def alloc_double_piecewise_linear(\n",
    "    yhat: np.ndarray,\n",
    "    tau1: float,\n",
    "    tau2: float,\n",
    "    tau3: float,\n",
    "    w1: float = 1.0,\n",
    "    w2: float = 2.0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Piecewise allocation:\n",
    "    - 0                     if yhat <= tau1\n",
    "    - linear 0 -> w1        if tau1 < yhat <= tau2\n",
    "    - linear w1 -> w2       if tau2 < yhat <= tau3\n",
    "    - w2                    if yhat > tau3\n",
    "    \"\"\"\n",
    "\n",
    "    w = np.zeros_like(yhat, dtype=float)\n",
    "\n",
    "    # Region 1: tau1 -> tau2\n",
    "    mask1 = (yhat > tau1) & (yhat <= tau2)\n",
    "    w[mask1] = w1 * (yhat[mask1] - tau1) / (tau2 - tau1)\n",
    "\n",
    "    # Region 2: tau2 -> tau3\n",
    "    mask2 = (yhat > tau2) & (yhat <= tau3)\n",
    "    # w[mask2] = w1 + (w2 - w1) * (yhat[mask2] - tau2) / (tau3 - tau2)\n",
    "    w[mask2] = w1\n",
    "\n",
    "    # Region 3: saturation\n",
    "    w[yhat > tau3] = w2\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Metric (matches competition score)\n",
    "# -----------------------------\n",
    "def adjusted_sharpe_metric(df: pd.DataFrame) -> dict:\n",
    "    w = df[\"position\"].values\n",
    "    rf = df[\"risk_free_rate\"].values\n",
    "    r = df[\"forward_returns\"].values\n",
    "\n",
    "    strategy_returns = rf * (1 - w) + w * r\n",
    "    strategy_excess = strategy_returns - rf\n",
    "\n",
    "    strat_excess_cum = np.prod(1.0 + strategy_excess)\n",
    "    strat_mean_excess = strat_excess_cum ** (1.0 / len(df)) - 1.0\n",
    "\n",
    "    strat_std = np.std(strategy_returns, ddof=1)\n",
    "    sharpe = strat_mean_excess / strat_std * np.sqrt(TRADING_DAYS_PER_YR)\n",
    "    strat_vol = strat_std * np.sqrt(TRADING_DAYS_PER_YR) * 100.0\n",
    "\n",
    "    market_excess = r - rf\n",
    "    market_excess_cum = np.prod(1.0 + market_excess)\n",
    "    market_mean_excess = market_excess_cum ** (1.0 / len(df)) - 1.0\n",
    "    market_std = np.std(r, ddof=1)\n",
    "    market_vol = market_std * np.sqrt(TRADING_DAYS_PER_YR) * 100.0\n",
    "\n",
    "    vol_penalty = 1.0 + max(0.0, strat_vol / market_vol - 1.2)\n",
    "    return_gap = max(0.0, (market_mean_excess - strat_mean_excess) * 100.0 * TRADING_DAYS_PER_YR)\n",
    "    return_penalty = 1.0 + (return_gap ** 2) / 100.0\n",
    "\n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "\n",
    "    return {\n",
    "        \"adjusted_sharpe\": float(adjusted_sharpe),\n",
    "        \"sharpe\": float(sharpe),\n",
    "        \"strategy_vol_pct\": float(strat_vol),\n",
    "        \"market_vol_pct\": float(market_vol),\n",
    "        \"vol_penalty\": float(vol_penalty),\n",
    "        \"return_penalty\": float(return_penalty),\n",
    "        \"strategy_mean_excess_daily_geo\": float(strat_mean_excess),\n",
    "        \"market_mean_excess_daily_geo\": float(market_mean_excess),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Rolling refit backtest\n",
    "# -----------------------------\n",
    "def rolling_backtest_double_piecewise(\n",
    "    full_train: pd.DataFrame,\n",
    "    feature_cols: list[str],\n",
    "    xgb_params: dict,\n",
    "    tau1: float,\n",
    "    tau2: float,\n",
    "    tau3: float,\n",
    "    half_life: float = 500.0,\n",
    "    refit_every: int = 100,\n",
    "):\n",
    "    df = full_train.reset_index(drop=True).copy()\n",
    "    n = len(df)\n",
    "\n",
    "    split = n // 2\n",
    "    oos_start = split\n",
    "\n",
    "    oos_pred = np.full(n, np.nan)\n",
    "    oos_pos = np.full(n, np.nan)\n",
    "\n",
    "    t = oos_start\n",
    "    while t < n:\n",
    "        train_end = t\n",
    "\n",
    "        X_train = df.loc[:train_end - 1, feature_cols]\n",
    "        y_train = df.loc[:train_end - 1, \"target\"]\n",
    "\n",
    "        idx = np.arange(train_end)\n",
    "        age = (train_end - 1) - idx\n",
    "        w_train = 0.5 ** (age / half_life)\n",
    "\n",
    "        model = XGBRegressor(**xgb_params)\n",
    "        model.fit(X_train, y_train, sample_weight=w_train, verbose=False)\n",
    "\n",
    "        chunk_end = min(t + refit_every, n)\n",
    "        X_chunk = df.loc[t:chunk_end - 1, feature_cols]\n",
    "        yhat_chunk = model.predict(X_chunk)\n",
    "\n",
    "        pos_chunk = alloc_double_piecewise_linear(\n",
    "            yhat_chunk,\n",
    "            tau1=tau1,\n",
    "            tau2=tau2,\n",
    "            tau3=tau3,\n",
    "            w1=1.0,\n",
    "            w2=1.25,\n",
    "        )\n",
    "\n",
    "        oos_pred[t:chunk_end] = yhat_chunk\n",
    "        oos_pos[t:chunk_end] = pos_chunk\n",
    "        t = chunk_end\n",
    "\n",
    "    oos = df.loc[oos_start:].copy()\n",
    "    oos[\"prediction\"] = oos_pred[oos_start:]\n",
    "    oos[\"position\"] = np.clip(oos_pos[oos_start:], MIN_INVESTMENT, MAX_INVESTMENT)\n",
    "\n",
    "    oos[\"strategy_returns\"] = (\n",
    "        oos[\"risk_free_rate\"] * (1 - oos[\"position\"])\n",
    "        + oos[\"position\"] * oos[\"forward_returns\"]\n",
    "    )\n",
    "\n",
    "    ic = pearsonr(oos[\"prediction\"], oos[\"target\"])[0]\n",
    "    metrics = adjusted_sharpe_metric(oos[[\"position\", \"forward_returns\", \"risk_free_rate\"]])\n",
    "    metrics[\"oos_ic_pred_vs_target\"] = float(ic)\n",
    "\n",
    "    return oos, metrics\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "final_features = sel_top_n_features(ranked_features, 15)\n",
    "\n",
    "xgb_params = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_estimators=1500,\n",
    "    learning_rate=0.005,\n",
    "    max_depth=3,\n",
    "    max_leaves=32,\n",
    "    min_child_weight=2,\n",
    "    gamma=0.0,\n",
    "    reg_alpha=0.01,\n",
    "    reg_lambda=0.5,\n",
    "    subsample=0.1,\n",
    "    colsample_bytree=0.5,\n",
    "    colsample_bylevel=0.5,\n",
    "    colsample_bynode=0.5,\n",
    "    tree_method=\"hist\",\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# Sensible starting values from your prediction distribution\n",
    "# tau1 = 0.0002  \n",
    "# tau2 = 0.0005  \n",
    "# tau3 = 1000000.02\n",
    "tau1 = 0.00001  \n",
    "tau2 = 0.0001  \n",
    "tau3 = 0.003\n",
    "  \n",
    "\n",
    "oos_df, metrics = rolling_backtest_double_piecewise(\n",
    "    full_train=full_train,\n",
    "    feature_cols=final_features,\n",
    "    xgb_params=xgb_params,\n",
    "    tau1=tau1,\n",
    "    tau2=tau2,\n",
    "    tau3=tau3,\n",
    "    half_life=500,\n",
    "    refit_every=100,\n",
    ")\n",
    "\n",
    "print(\"Double piecewise linear allocation metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7fc834ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['M4', 'S8', 'V13', 'V9', 'M12', 'S3', 'S1', 'I3', 'V5', 'P6', 'E19',\n",
       "       'V10', 'M17', 'S5', 'P13'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_final.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
